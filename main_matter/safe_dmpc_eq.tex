\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter[Resilient Primal Decomposition-based dMPC for scarce systems]{Resilient \\Primal Decomposition-based \\distributed \\Model Predictive Control\\ for scarce systems}\label{sec:safe_pddmpc_eq}
\epigraph{\centering So you tell me 'trust me' \\I can trust you\\ Just let me show you \\But I gotta work it out in a shadow of doubt \\'Cause I don't know if I know you}
{\textit{Lie}\\\textsc{Kevin Moore}}

In this chapter, we focus on the analysis of the primal decomposition-based \dmpc{} problem for scarce systems when in the presence of the attack chosen in~\S\ref{sec:anomalous}.

With the knowledge acquired after the analysis, we propose a method to detect the attacks and mitigate its effects.

\minitoc


\section{Scarce systems under attack}\label{sec:analys-scarce-syst}

Different from~\cite{VelardeEtAl2018,MaestreEtAl2021} which propose robust solutions, we propose a resilient \dmpc{} based on a hybrid of analytical/learning active detection method.
For this end, we begin with an analysis of the system under attack.

\subsection{Scarce Systems}\label{sec:scarce-systems}
First we recall the monolithic \mpc{} equivalent problem~\eqref{eq:qp_standard_form}, once more reproduced for the reader's convenience,
\begin{equation}
  \label{eq:qp_standard_form_again}
  \tag{\ref*{eq:qp_standard_form}}
  \begin{aligned}
    \begin{matrix}
      \minimize\limits_{\vec{U}[k]} &
                                                 \frac{1}{2}\norm{\vec{U}[k]}^{2}_{H} + {\vec{f}[k]}^{T}\vec{U}[k] &\\
      \mathrm{subject~ to} &
                             \bar{\Gamma}\vec{U}[k]\preceq {\vec{U}}_{\text{max}}
    \end{matrix}
  \end{aligned},
\end{equation}
and the local problems~\eqref{eq:DOP_local} solved in the primal decomposition, also reproduced,

\begin{equation}
  \label{eq:DOP_local_reprise}
  \tag{\ref*{eq:DOP_local}}
  \bar{J}_{i}^{\star}(\thetaik)=
  \begin{matrix}
    \minimize\limits_{\vec{U}_{i}[k]}&\obji=\frac{1}{2}\norm{\vec{U}_{i}[k]}^{2}_{H_{i}} + {\vec{f}_{i}[k]}^{T}\vec{U}_{i}[k]\\
    \mathrm{subject~ to} & \bar{\Gamma}_{i}\vec{U}_{i}[k] \preceq \thetaik:\lambdaik
  \end{matrix}.
\end{equation}

The unconstrained version of problems~\eqref{eq:qp_standard_form_again} and~\eqref{eq:DOP_local_reprise} are
\begin{align}
  \label{eq:qp_standard_form_unconstrained}
  \begin{aligned}
    \begin{matrix}
      \minimize\limits_{\vec{U}[k]} &
                                                 \frac{1}{2}\norm{\vec{U}[k]}^{2}_{H} + {\vec{f}[k]}^{T}\vec{U}[k] &\\
    \end{matrix},
  \end{aligned}\\
  \label{eq:DOP_local_unconstrained}
  \begin{aligned}
    \begin{matrix}
    \minimize\limits_{\vec{U}_{i}[k]}&\frac{1}{2}\norm{\vec{U}_{i}[k]}^{2}_{H_{i}} + {\vec{f}_{i}[k]}^{T}\vec{U}_{i}[k]\\
    \end{matrix},
  \end{aligned}
\end{align}
and have analytical solutions~\cite{BoydVandenberghe2004}
\begin{align}
  \label{eq:qp_standard_form_unconstrained_solution}
  \vec{U}_{\text{unc}}^{\star}[k]=-H^{-1}\vec{f}[k],\\
  \label{eq:DOP_local_unconstrained_solution}
  \vec{U}_{i_{\text{unc}}}^{\star}[k]=-H_{i}^{-1}\vec{f}_{i}[k].
\end{align}

We call a system scarce when its unconstrained solution $\vec{U}_{\text{unc}}^{\star}[k]$ lies outside the bounds of the polytope formed by the constraints for all $k$, i.e.,
\begin{equation}
\bar{\Gamma}\vec{U}_{\text{unc}}[k]\succ {\vec{U}}_{\text{max}}, \forall k.
\end{equation}

Furthermore, in this chapter, we assume that for all sub-systems, their unconstrained solutions $\vec{U}_{i_{\text{unc}}}^{\star}[k]$ neither respect the local constraints
\begin{equation}
\bar{\Gamma}_{i}\vec{U}_{i_{\text{unc}}}^{\star}[k]\succ \thetaik, \forall i\in\set{M}, \forall k.
\end{equation}

This means that the optimal solution would need more resources (more than $\vec{U}_{\max}$).
So the solution to those \qp{} problems is the projection of the unconstrained solutions onto the polytope. Thus, projecting onto the perimeter of the polytope.

We assume that given projection solves the inequality constrained problem equivalent to an equality constraint problem
\begin{equation}
  \label{eq:qp_standard_form_equality}
  \begin{aligned}
    \begin{matrix}
      \minimize\limits_{\vec{U}[k]} &
                                                 \frac{1}{2}\norm{\vec{U}[k]}^{2}_{H} + {\vec{f}[k]}^{T}\vec{U}[k] &\\
      \mathrm{subject~ to} &
                             \bar{\Gamma}_{\text{eq}}\vec{U}[k]= {\vec{U}}_{\text{max}}
    \end{matrix}
  \end{aligned}.
\end{equation}

And as one could perceive, it leads to a decomposition exactly as in \S\ref{sec:example-interpr}
with local problems~\eqref{eq:example_local_problem} and negotiation equation~\eqref{eq:example_projectedSubgradient_lambda}, reproduced here,
\begin{equation}
  \begin{matrix}
  \label{eq:example_local_problem_reprise}
  \tag{\ref*{eq:example_local_problem}}
    \minimize\limits_{\vec{U}_{i}[k]}&\obji=\frac{1}{2}\norm{\vec{U}_{i}[k]}^{2}_{H_{i}} + {\vec{f}_{i}[k]}^{T}\vec{U}_{i}[k]\\
    \mathrm{subject~ to} & \bar{\Gamma}_{i_{\text{eq}}}\vec{U}_{i}[k] = \thetaik:\lambdaik
  \end{matrix}
\end{equation}
\begin{equation}
  \label{eq:example_projectedSubgradient_lambda_reprise}
  \tag{\ref*{eq:example_projectedSubgradient_lambda}}
 \thetai\pplusone=\thetai\p+\rho\p\left(\lambdai\p-\frac{1}{M}\sum_{j=1}^{M}\vec{\lambda}_j\p\right),\forall i\in\set{M}.
\end{equation}

These are the equations we will use for our analysis.

\begin{remark}
  Observe that not necessarily $\bar{\Gamma}_{\text{eq}}$ (or $\bar{\Gamma}_{\text{eq}}$) is the same as $\bar{\Gamma}$ (or $\bar{\Gamma}_{i}$).
  The constraints in~\eqref{eq:qp_standard_form_again} and~\eqref{eq:DOP_local_reprise}, can be interpreted as the intersection of the halfspaces described by the rows of $\bar{\Gamma}$ (or $\bar{\Gamma}_{i}$) and $\vec{U}_{\max}$ (or $\thetaik$), i.e., the halfspaces
  ${\set{S}_i=\setbuild{\vec{x}}{\elem[i,\star]{{\bar{\Gamma}}}\vec{x}\leq\elem[i,\star]{\vec{U}_{\max}}}}$, and the intersection ${\set{S}=\setbuild{\vec{x}}{\bar{\Gamma}\vec{x}\preceq\vec{U}_{\max}}=\bigcap\limits_{i=1}^{\card\vec{U}_{\max}}}\set{S}_{i}$ (and conversely for $\bar{\Gamma}_{i}$ and $\thetaik$).

  So, to find the equivalent $\bar{\Gamma}_{\text{eq}}$ (or $\bar{\Gamma}_{i_{\text{eq}}}$) for the equality constraint problem, only the active halfspaces are kept and transformed into intersection of hyperplanes (equality constraint).
  This computation of active sets may be costly depending on the number of halfspaces used and the dimensions of the variables.

  If we assume the feasible regions of $\bar{\Gamma}$ (and $\bar{\Gamma}_{i}$) to form a cone, if the systems is scarce, $\bar{\Gamma}_{\text{eq}}=\bar{\Gamma}$ (and $\bar{\Gamma}_{i_\text{eq}}=\bar{\Gamma}_i$)\footnote{This can be easily proven geometrically, when a point do not respect any of the inequality constraints the projection onto the cone will be its apex (which coincides with the intersection of the equivalent hyperplanes).}.

One way to ensure this, is to make the original constraint~\eqref{eq:linear_constraint} to have at most as many rows as columns, i.e., $\card{\vec{u}_{\max}}\leq n_{u}$, although it may be a little restrictive.

So, from now on, we make this assumption and use $\bar{\Gamma}$ and $\bar{\Gamma}_{i}$.

  % For our study cases where $\Gamma$ has all its elements positive (such as postive weighted sum of all inputs), and block diagonal structure with repeating pattern (from the \mpc{} definition~\eqref{eq:construction_Gamma}), we assume we can find the corresponding problem and change $\bar{\Gamma}$ (and $\bar{\Gamma}_i$) accordingly, by removing the proper rows.
\end{remark}

\subsection{Why scarce systems?}\label{sec:why-scarce-systems}

When a system does not suffer from scarcity, the solution of the unconstrained problem~\eqref{eq:qp_standard_form_unconstrained_solution} respects the inequalities, i.e.,
\begin{equation}
\bar{\Gamma}\vec{U}_{\text{unc}}[k]\preceq {\vec{U}}_{\text{max}},\forall k.
\end{equation}

This way, the elements of $\vec{U}_{\text{unc}}$ lie inside the polytope and none of the constraints is active.
As a consequence, no projection onto the polytope is needed, since it would result in the same point.
That means the solution of the constrained problem is the same as the unconstrained, so the problem is in fact unconstrained.

Instead of needing to solve the local problems~\eqref{eq:DOP_local_reprise} and use a negotiation to decompose the system, the problem doesn't need coordination.
The problem is solved as shown in \S\ref{sec:uncoupled_problems}, i.e., each subsystem solves~\eqref{eq:qp_standard_form_unconstrained_solution} and the solution is already given.

This case is uninteresting because since the agents do not need to compromise to find a consensus, there is no incentive to attack the system to gain more resources since it would not be used and it would not affect the others, who would be nevertheless satisfied.
Furthermore, since no negotiation is needed, there is no communication and the agent would need to find other ways to attack the system.

When the system suffer from scarcity the agents are forced to compete and compromise, what may result in attacks as shown in \S\ref{sec:primal_decomposition} and their dire consequences.

\subsection{Analysis of local problems}\label{sec:analysis-local-problems}
Since the optimization problems~\eqref{eq:example_local_problem_reprise} are \qp{} problems with equality constraints, it is known that they have analytical solution~\cite{BoydVandenberghe2004}.
To find this solution we usually use the lagrange duality, so $\lambdaik$ will also have analytical solution in this case.
First, to simplify the notation we define a function $\equalityfunctionname:\R^{\card\vec{U}_{i}[k]}\times \R^{\card\thetaik}$, defined as
\begin{equation}
  \label{eq:lagrangian_equality}
  \equalityfunction=\bar{\Gamma}_{i}\vec{U}_i[k]-\thetaik,
\end{equation}
to put the problem in standard form for equality constraint \qp{} programs:
\begin{equation}
  \begin{matrix}
  \label{eq:local_problem_equality_standard_notation}
    \minimize\limits_{\vec{U}_{i}[k]}&\obji(\vec{U}_{i}[k],\thetaik)\\
    \mathrm{subject~ to} & \equalityfunction= 0:\lambdaik
  \end{matrix}
\end{equation}

With this form, we define the \emph{Lagrangian} function $\lagrangianname : \R^{\card\vec{U}_{i}[k]}\times \R^{\card\lambdaik} \times \R^{\card\thetaik} \to \R$ as
\begin{equation}
  \label{eq:lagrangian_equality}
  \lagrangian=\obji+\lambdaik^T\equalityfunction,
\end{equation}
which embeds the equality constraint into the objective function using $\lambdaik$, called the lagrangian multiplier for equality.
In this chapter, since there is only one type of lagrangian multiplier, we will call it the dual variable.
\begin{remark}
  Observe that by complementary slackness, ${\lambdaikstar}^T\equalityfunctionname(\vec{U}_{i}^{\star}[k],\thetaik)$ will always be zero.
 If any element of $\equalityfunctionname(\vec{U}_{i}^{\star}[k],\thetaik)$ is different from zero, the corresponding element on the dual variable $\lambdaikstar$ will be zero, indicating the constraint is active.
\end{remark}

Then we define the dual function $\dualfunctionname:\R^{\card\lambdaik} \times \R^{\card\thetaik} \to \R$ which is calculated by solving
\begin{equation}
  \label{eq:dual_function_equality}
  \dualfunction=\inf_{\vec{U}_{i}[k]\in\set{F}} \lagrangian,
\end{equation}
where $\set{F}=\dom \equalityfunctionname=\bigcap\limits_{i=1}^{\card\vec{U}_{\max}}\set{P}_{i}$, with $\set{P}_{i}$ being the hyperplanes
${\set{P}_i=\setbuild{\vec{x}}{\elem[i,\star]{{\bar{\Gamma}}}\vec{x}=\elem[i,\star]{\vec{U}_{\max}}}}$.

The solution to $\dualfunction$ is given by the first order \KKT{} optimality condition
\begin{equation}
\nabla_{\vec{U}_{i}[k]}\lagrangian=0
\end{equation}
which results in
\begin{equation}
\vec{U}_{i}[k]=-H_{i}^{-1}(-\bar{\Gamma}_{i}^{T}\lambdai[k]-\vec{f}_{i}[k]).
\end{equation}

Substituting it in $\lagrangian$ yields
\begin{equation}
  \label{eq:dual_function_solution_lambda_theta}
\dualfunction=-\frac{1}{2}\lambdaik^{T}\linearcoefi\lambdaik-\lambdaik^{T}\bar{\Gamma}_{i}H_{i}^{-1}\vec{f}[k]-\lambdaik^{T}\thetaik.
\end{equation}

As presented in~\cite{BoydVandenberghe2004} and as we can see from inspecting~\eqref{eq:dual_function_solution_lambda_theta}, $\dualfunction$ is concave. Thus, we can find $\lambdaikstar$ by solving
\begin{equation}
  \begin{matrix}
    \lambdaikstar=\argmax\limits_{\lambdaik}\maximize\limits_{\lambdaik}&\dualfunction.
  \end{matrix}
\end{equation}

Similarly, using first order \KKT{} condition we make the gradient of $\dualfunction$ vanish
\begin{equation}
\nabla_{\lambdaik}\dualfunction=0,
\end{equation}
which yields
\begin{equation}
  \label{eq:lambda_function_theta}
  \lambdaik=-\Plin\thetaik-\sik,
\end{equation}
where $\Plin={(\linearcoefi)}^{-1}$ and $\sik=\Plin\bar{\Gamma}_{i}H_{i}^{-1}\vec{f}_{i}[k]$.

As we expected, the value of $\lambdaik$ depends on $\thetaik$.
Furthermore, because of the quadratic form, the solution is an affine function.

\begin{remark}
  One can notice, as foreshadowed in \S\ref{sec:attack-interest}, that the solution embeds information not only of the objective function and the reference and state (presence of $H_{i}$ and $\vec{f}_{i}[k]$), but also of the constraints (presence of $\bar{\Gamma}_{i}$ and $\thetaik$).
  So, changes in these parameters affect the resulting value of $\lambdaik$.
\end{remark}

\subsection{Analysis of negotiation}\label{sec:analysis-negotiation}
Using the results of equation~\eqref{eq:example_projectedSubgradient_lambda_reprise}, we can observe the dynamics of $\thetai$ and $\lambdai$ during a negotiation, and from this get some insight for future detection and mitigation methods.

\subsubsection{Dynamics of $\lambdai$}
We can analyze the dynamics of $\lambdai$ by substituting
~\eqref{eq:example_projectedSubgradient_lambda_reprise} into~\eqref{eq:lambda_function_theta}, which yields
\begin{equation}
  \label{eq:lambda_dynamics}
\lambdai\pplusone=-\Plin\left(\thetai\p+\rho\p\left(\lambdai\p-\frac{1}{M}\sum_{j=1}^{M}\vec{\lambda}_j\p\right)\right)-\sik.
\end{equation}

We can distribute the terms, resulting in
\begin{equation}
  \label{eq:lambda_dynamics_with_P_and_s}
\lambdai\pplusone=-\Plin\rho\p\lambdai\p+\Plin\rho\p\frac{1}{M}\sum_{j=1}^{M}\vec{\lambda}_j\p-\Plin\thetai\p-\sik.
\end{equation}
From inspection of~\eqref{eq:lambda_function_theta}, the last term ($-\Plin\theta_{i}\p-\sik$) is equals to $\lambdai\p$, substituting it into~\eqref{eq:lambda_dynamics_with_P_and_s} and using a matrix form, we have
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing}
  \vec{\lambda}\pplusone=\mathcal{A}_{\lambda}\vec{\lambda}\p,
\end{equation}
with
\begin{equation}
\mathcal{A}_{\lambda}=\left[
\begin{matrix}
I-\frac{M-1}{M}\rho\p \Plin[1] & \frac{1}{M}\rho\p \Plin[1]&\dots&\frac{1}{M}\rho\p \Plin[1]\\
\frac{1}{M}\rho\p \Plin[2]&I-\frac{M-1}{M}\rho\p \Plin[2]&\dots&\frac{1}{M}\rho\p \Plin[2]\\
\vdots&\vdots&\ddots&\vdots\\
\frac{1}{M}\rho\p \Plin[M]&\frac{1}{M}\rho\p \Plin[M]&\dots&I-\frac{M-1}{M}\rho\p \Plin[M]\\
\end{matrix}
\right],
\end{equation}
which is a \dt{} homogeneous system.
The system varies with time, since $p$ changes, but usually it vanishes as it approaches infinity, as seen in~\eqref{eq:rho_update}.
So, we can use Lyapunov criteria~\cite[\S8.6]{Hespanha2009}, i.e., the eigenvalues of $\mathcal{A}_{\lambda}$, must be inside the unit circle for the system to be stable.
For smaller systems, we can use Jury's criteria to test the system~\cite{Jury1962}.

Analyzing $\mathcal{A}_{\lambda}$ a little further, we see that the sum of its columns is equal to one\footnote{$I+\frac{M-1}{M}\rho\p \Plin+\sum\limits_{j=1}^{M-1}\frac{1}{M}\rho\p \Plin=I$},
which means that $1$ is a right eigenvalue of $\mathcal{A}_{\lambda}$:
\begin{equation*}
  \mathcal{A}_{\lambda}\1=\1
\end{equation*}
So, supposing the other eigenvalues are inside the unit circle, the system converges to a state $\1c$, i.e., where all lambdas are equal~\cite{GarinSchenato2010,XiaoEtAl2007}.
On the other hand, the sum of rows is not equal, which means the sum of all states is not constant, or as said in the consensus community, the mass is not conserved.
This way, we could not use average consensus strategies, unless all subsystems had the same $\Plin$, which would render the problem extremely uninteresting (solution is divide the resources equally).

If we suppose the system is under attack, we can use the attack model~\eqref{eq:linear_attack}, reproduced here,
\begin{equation}
  \label{eq:linear_attack_reprise}
  \tag{\ref*{eq:linear_attack}}
  \tilde{\vec{\lambda}}_{i}[k]=\Tik\lambdaik.
\end{equation}
Any agent could be the perpetrator of the attack (potentially multiple agents may attack simultaneously), so we use the model for all agents.
Remaking the last steps with this attack model we can finally have the dynamics of the attacked system:
% \begin{equation}
% \tilde{\lambdai}\pplusone=\Tik\left(-\Plin\left(\thetai\p+\rho\p\left(\tilde{\vec{\lambda}}_{i}\p-\frac{1}{M}\sum_{j=1}^{M}\tilde{\vec{\lambda}}_{j}\p\right)\right)-\sik\right).
% \end{equation}
% \begin{equation}
%   \label{eq:lambda_dynamics_with_P_and_s}
% \tilde{\lambdai}\pplusone=-\Tik\Plin\rho\p\tilde{\vec{\lambda}}_{i}\p+\Tik\Plin\rho\p\frac{1}{M}\sum_{j=1}^{M}\tilde{\vec{\lambda}}_j\p+\Tik\left(-\Plin\thetai\p-\sik\right).
% \end{equation}
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing_under_attack}
  \tilde{\vec{\lambda}}\pplusone=\tilde{\mathcal{A}}_{\lambda}\tilde{\vec{\lambda}}\p,
\end{equation}
with
\begin{equation}
\tilde{\mathcal{A}}_{\lambda}=\left[
\begin{matrix}
I-\frac{M-1}{M}\rho\p \Tik[1]\Plin[1] & \frac{1}{M}\rho\p \Tik[1]\Plin[1]&\dots&\frac{1}{M}\rho\p \Tik[1]\Plin[1]\\
\frac{1}{M}\rho\p \Tik[2]\Plin[2]&I-\frac{M-1}{M}\rho\p \Tik[2]\Plin[2]&\dots&\frac{1}{M}\rho\p \Tik[2]\Plin[2]\\
\vdots&\vdots&\ddots&\vdots\\
\frac{1}{M}\rho\p \Tik[M]\Plin[M]&\frac{1}{M}\rho\p \Tik[M]\Plin[M]&\dots&I-\frac{M-1}{M}\rho\p \Tik[M]\Plin[M]\\
\end{matrix}
\right],
\end{equation}

As we have attested before through the simple example in \S\ref{sec:attack-interest}, the cheating matrices $\Tik$ can change the eigenvalues of the matrix, making the system oscillate and not converge to the optimal value.

\begin{remark}
  Observe that as $p$ increases, $\rho\p$ vanishes, and as consequence the matrix $\mathcal{A}_{\lambda}$ presents less and less influence of $\Plin$ (and $T_i[k]$) and its eigenvalues approach $1$.
  This measure ensures convergence in finite time but with a loss in accuracy.
\end{remark}


\subsubsection{Dynamics of $\thetai$}
Conversely, we can analyze the dynamics of $\thetai$ by
taking the negotiation equation~\eqref{eq:example_projectedSubgradient_lambda_reprise},
and substituting $\lambdaik$ by~\eqref{eq:lambda_function_theta}, which yields
\begin{equation}
  \label{eq:negotiation_equation_substituting}
 \thetai\pplusone=\thetai\p+\rho\p\left((-\Plin\thetai\p-\sik)-\frac{1}{M}\sum_{j\in\set{M}}(-\Plin[j]\vec{\theta}_{j}\p-\vec{s}_{j}[k])\right),\forall i\in\set{M}.
\end{equation}

As we can see, we can separate the function into two parts, one which depends on $\thetai$ and other which do not:
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing}
 \thetai\pplusone=\thetai\p+\rho\p\left(-\Plin\thetai\p+\frac{1}{M}\sum_{j\in\set{M}}\Plin[j]\vec{\theta}_{j}\p\right)+\rho\p\left(-\sik+\frac{1}{M}\sum_{j\in\set{M}}\vec{s}_{j}[k]\right),\forall i\in\set{M}.
\end{equation}

We can finally write it in matrix form:
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing}
  \vec{\theta}\pplusone=\mathcal{A}_{\theta}\vec{\theta}\p+\mathcal{\vec{B}}_{\theta}[k]
\end{equation}
where
\begin{equation}
\mathcal{A}_{\theta}=\left[
\begin{matrix}
I-\frac{M-1}{M}\rho\p \Plin[1] & \frac{1}{M}\rho\p \Plin[2]&\dots&\frac{1}{M}\rho\p \Plin[M]\\
\frac{1}{M}\rho\p \Plin[1]&I-\frac{M-1}{M}\rho\p \Plin[2]&\dots&\frac{1}{M}\rho\p \Plin[M]\\
\vdots&\vdots&\ddots&\vdots\\
\frac{1}{M}\rho\p \Plin[1]&\frac{1}{M}\rho\p \Plin[2]&\dots&I-\frac{M-1}{M}\rho\p \Plin[M]\\
\end{matrix}
\right]
\end{equation}
\begin{equation}
\mathcal{\vec{B}}_{\theta}[k]=\left[
\begin{matrix}
-\frac{M-1}{M}\rho\p \vec{s}_{1}[k]+\frac{1}{M}\rho\p \vec{s}_{2}[k]\dots-\frac{1}{M}\rho\p \vec{s}_{M}[k]\\
\frac{1}{M}\rho\p \vec{s}_{1}[k]-\frac{M-1}{M}\rho\p \vec{s}_{2}[k] \dots-\frac{1}{M}\rho\p \vec{s}_{M}[k]\\
\vdots\\
\frac{1}{M}\rho\p \vec{s}_{1}[k]+\frac{1}{M}\rho\p \vec{s}_{2}[k]\dots-\frac{M-1}{M}\rho\p \vec{s}_{M}[k]\\
\end{matrix}
\right]
\end{equation}
which is a \dt{} system with forced input ($\mathcal{\vec{B}}_{\theta}$).
We can analyze the stability of $\mathcal{A}_{\theta}$ with the same tools used for $\mathcal{A}_{\lambda}$.
For it to be stable, the eigenvalues must reside inside the unit circle.

We can also do the same analysis of the sum of the rows and columns.
If we observe the sum of rows, we acknowledge it is equal to $1$.
So, $1$ is one of the left eigenvalues of the matrix:
\begin{equation}
  \label{eq:1}
  \1^{T}\mathcal{A}_{\theta}=\1^{T}
\end{equation}
As expected the system conserves mass, i.e., the $\theta_{i}$ must respect the global equality constraint.
This way, the linear dependence between $\theta_{i}$ is explicit.
If we analyze the columns, on the other hand, we see that they not sum to $1$.
This means, if the system converges, not necessarily it will converge in such way the $\theta_{i}$ are equal.
Again, it would be the case if the $\Plin$ were equal.

As we can see, the values of the eigenvalues depend only on the values of $\Plin$ and $\rho\p$.
Since the $\Plin$ are system parameters, the designer of the control can only tune $\rho\p$.

Once $\rho\p$ is well tuned for the corresponding system, the matrix $\mathcal{A}_{\theta}$ evolves always in the same way.
So if there is a change in this evolution, it means an the system presents an anomalous behavior.

As can be observed, any disturbance in $\Plin$ (change in objective function or constraints) changes the values of $\mathcal{A}_{\theta}$, thus the system's dynamic. This change may destabilize it if drives the eigenvalues outside the unit circle, as shown in \S\ref{sec:attack-interest}.
On the other hand, perturbations in $\sik$ (modification of objective function, constraints or state/reference) may change the steady state value (change in $\mathcal{\vec{B}}_{\theta}[k]$).

If we suppose the system is under attack, we use again the attack model~\eqref{eq:linear_attack_reprise}.
We suspect all agents and we modify~\eqref{eq:lambda_function_theta} accordingly. Then we substitute it into~\eqref{eq:example_projectedSubgradient_lambda_reprise}.
Then, we can have the dynamics of an attacked system
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing_tilde}
  \vec{\theta}\pplusone=\tilde{\mathcal{A}}_{\theta}[k]\vec{\theta}\p+\tilde{\mathcal{\vec{B}}}_{\theta}[k]
\end{equation}
where
\begin{equation}
\tilde{\mathcal{A}_{\theta}}[k]=\left[
\begin{matrix}
I-\frac{M-1}{M}\rho\p \Tik[1]\Plin[1] & \frac{1}{M}\rho\p \Tik[2]\Plin[2]&\dots&\frac{1}{M}\rho\p \Tik[M]\Plin[M]\\
\frac{1}{M}\rho\p \Tik[1]\Plin[1]&I-\frac{M-1}{M}\rho\p \Tik[2]\Plin[2]&\dots&\frac{1}{M}\rho\p \Tik[M]\Plin[M]\\
\vdots&\vdots&\ddots&\vdots\\
\frac{1}{M}\rho\p \Tik[1]\Plin[1]&\frac{1}{M}\rho\p \Tik[2]\Plin[2]&\dots&I-\frac{M-1}{M}\rho\p \Tik[M]\Plin[M]\\
\end{matrix}
\right]
\end{equation}
\begin{equation}
\tilde{\mathcal{\vec{B}}}_{\theta}[k]=\left[
\begin{matrix}
-\frac{M-1}{M}\rho\p \Tik[1]\vec{s}_{1}[k]+\frac{1}{M}\rho\p \Tik[2]\vec{s}_{2}[k]\dots-\frac{1}{M}\rho\p \Tik[M]\vec{s}_{M}[k]\\
\frac{1}{M}\rho\p \Tik[1]\vec{s}_{1}[k]-\frac{M-1}{M}\rho\p \Tik[2]\vec{s}_{2}[k] \dots-\frac{1}{M}\rho\p \Tik[M]\vec{s}_{M}[k]\\
\vdots\\
\frac{1}{M}\rho\p \Tik[1]\vec{s}_{1}[k]+\frac{1}{M}\rho\p \Tik[2]\vec{s}_{2}[k]\dots-\frac{M-1}{M}\rho\p \Tik[M]\vec{s}_{M}[k]\\
\end{matrix}
\right]
\end{equation}


As in the case with $\mathcal{A}_{\theta}$, by adjusting $\Tik$, an attacker can change the eigenvalues of $\mathcal{A}_{\lambda}$.
It can also be adjusted to drive the negotiation to a new point, modifying $\mathcal{B}$, but as we see, it modifies at the same time the dynamics.
This way, the attackers need to compromise between their greediness and not breaking the system, which is worse for every agent, the attackers included.

As one can notice, we could create an anomaly detector by supervising the matrix $\mathcal{A}_{\theta}$.
Once there is a change in some of the variables (or in its eigenvalues) an anomalous behavior is detected.

Unfortunately, this method gives us only a detection, but not necessarily from which agent the anomaly comes, what could useful for a mitigation algorithm.
Besides, if we were to estimate all elements of $\mathcal{A}_{\theta}$, we would need to estimate $M^{2}{(\card\thetai)}^{2}$ elements\footnote{$M^{2}$ blocks of size ${\card\thetai}$}.
Exploiting the known structures of the matrix and their blocks, we could reduce to at least $2M{(\frac{\card\thetai+{\card\thetai}^{2}}{2})}$ elements\footnote{($M$ diagonal + $M$ off diagonal) symmetric blocks of size ${\card\thetai}$ (only upper triangle)}.

In the next section, we derive a detection mechanism, also based on estimation, which can not only detect the attacks, but also isolate the perpetrators, estimating only half of the elements cited.

\section{A detection mechanism}\label{sec:detection-mechanism}

Using~\eqref{eq:linear_attack_reprise} we can rewrie~\eqref{eq:lambda_function_theta} as
\begin{equation}
  \label{eq:lambda_function_theta_tilde}
\tilde{\vec{\lambda}}_{i}[k]=-\tilde{P}_{i}\thetaik-\tilde{\vec{s}}_{i}[k],
\end{equation}
where $\Plintilde=\Tik\Plin$ and $\tilde{\vec{s}}_{i}[k]=\Tik\sik$.
This way we can interpret as the cheating matrix $\Tik$ alters both $\\Plin$ and $\siktilde[1]$


% \begin{figure}[h]
%   \centering
%   \begin{subfigure}{.45\textwidth}
%     \includegraphics[width=\textwidth]{../img/original-minimum.png}
%     \caption{Original minimum.}
%     \label{fig:first}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.45\textwidth}
%     \includegraphics[width=\textwidth]{../img/new-minimum-selfish.png}
%     \caption{Minimum after attack.}
%     \label{fig:second}
%   \end{subfigure}
%   \caption{Effects of non-conforming behaviors on optimal value. \todo{Refaire les images}}
%   \label{fig:figures}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \begin{subfigure}{0.45\textwidth}
%     \includegraphics[width=\textwidth]{../img/ignoreX.png}
%     \caption{Optimal value after ignoring attacker.}
%     \label{fig:third}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.45\textwidth}
%     \includegraphics[width=\textwidth]{../img/correctX.png}
%     \caption{Optimal value after trying to recover original behavior.}
%     \label{fig:third}
%   \end{subfigure}

%   \caption{Recovery options.}\label{fig:figures}
% \end{figure}

% \section{Study case}
% \label{sec:study-case}

% Model 3R-2C \cite{GoudaEtAl2002}
% \begin{figure}[h]
%   \centering
%   \begin{circuitikz}[european]
%     \draw (0,0) node[tlground]{} to[isource, l=$P^{\text{heat}}$] ++(0,2) to[short, -*] ++(1.5,0) coordinate (a);

%     \draw (a) node[above]{$T^{\text{in}}$}  to[C=$C^{\text{air}}$] ++(0,-2) node[tlground]{};
%     \draw (0,-3) node[tlground]{} to[isource, l=$I^{\text{sol}}$] ++(0,2)
%     to[short, -*] ++(1.5,0) coordinate (b);
%     \draw (b) to[C=$C^{\text{walls}}$] ++(0,-2) node[tlground]{};

%     \draw (a) -- ++(2,0) coordinate (c) -- ++(0,-.5) to[R=$R^{\text{iw/ia}}$] ++(0,-2) -- ++(0,-.5) coordinate (d);

%     \draw (b) node[above]{$T^{\text{walls}}$} to[short,-*] (d);

%     \draw (c) --  ++(2.5,0) -- ++(0,-.5) to[R=$R^{\text{oa/ia}}$] ++(0,-2) -- ++(0,-.5) coordinate (e);

%     \draw (d) to[R=$R^{\text{ow/oa}}$] (e) to[battery,l=$T^{\text{out}}$] ++(0,-2) node[tlground]{};
%   \end{circuitikz}
%   \caption{Thermic Model 3R-2C of a room.}
%   \label{fig:3R2C_model}
% \end{figure}

% The state-space model of each subsystem is given by:
% \begin{equation}
%   \begin{matrix}
%     \label{eq:systems_cont}
%     \dot{\vec{x}}_{i}(t)  &=&{A_{c}}_{i}\vec{x}_{i}(t) &+& {B_{c}}_{i}\vec{u}_{i}(t)\\
%     \vec{y}_{i}(t)        &=&{C_{c}}_{i}\vec{x}_{i}(t) &&
%   \end{matrix},
% \end{equation}
% where
% \begin{equation}
%   \label{eq:4}
%   \begin{matrix}
%     A_{\mathrm{c}_{i}}=\left[
%     \begin{matrix}
%       -\frac{1}{C^{\text{walls}}_{i}R^{\text{oa/ia}}_{i}}-\frac{1}{C^{\text{walls}}_{i}R^{\text{iw/ia}}_{i}}& \frac{1}{C^{\text{walls}}_{i}R^{\text{iw/ia}}_{i}}\\
%       \frac{1}{C^{\text{air}}_{i}R^{\text{iw/ia}}_{i}} &-\frac{1}{C^{\text{air}}_{i}R^{\text{ow/oa}}o_{i}}-\frac{1}{C^{\text{air}}_{i}R^{\text{iw/ia}}_{i}}
%     \end{matrix}\right]\\
%     \begin{matrix}
%       B_{\mathrm{c}_{i}}=\left[
%       \begin{matrix}  \frac{10}{C^{\text{walls}}_{i}}& 0\end{matrix}
%                                                        \right]\T&C_{\mathrm{c}_{i}}=\left[\begin{matrix}1 & 0\end{matrix}\right]
%     \end{matrix}
%   \end{matrix}
% \end{equation}
% where ${\vec{x}_{i}=[{{x}_{A}}_{i}\T\ {{x}_{W}}_{i}\T]\T}$. ${x_A}_i$ and ${x_W}_i$ are the mean temperatures of the air and walls inside room~$i$. $\vec{u}_{i}$ is the input (the heating power)
% for the corresponding room. The inputs are constraint by ${\sum_{i=1}^{4}\vec{u}_{i}(t)\preceq 4\mathrm{kW}}$.

% \begin{table}[b]
%   \centering
%   \caption{Model Parameters}\label{tab:modelParamMeaning}
%   \begin{tabular}[b]{cl}
%     \toprule
%     Symbol&Meaning\\
%     \midrule
%     $C^{\text{air}}_{i}$  &Heat Capacity of Inside Air\\
%     $C^{\text{walls}}_{i}$ &Heat Capacity of External Walls\\
%     $R^{\text{iw/ia}}_{i}$ &Resist. Between Inside Air and Inside Walls\\
%     $R^{\text{ow/oa}}_{i}$ &Resist. Between Outside Air and Outside Walls\\
%     $R^{\text{oa/ia}}_{i}$ &Resist. Between Inside and Out.\ Air (from windows)\\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{table}[b]
%   \centering
%   \caption{
%     Parameters for each agent}\label{tab:modelParam}
%   \begin{tabular}[t]{cccccc} \toprule
%     Symbol& I & II & III & IV &Unit\\
%     \midrule
%     $C^{\text{walls}}$   &$5.4$&$4.9$&$4.7$&$4.7$ &$10^{4}\mathrm{J/K}$ \\
%     $C^{\text{air}}$     &$7.5$ &$8.4 $&$8.2$ &$7.7$&$10^{4}\mathrm{J/K}$  \\
%     $R^{\text{oa/ia}}$   &$5.2$&$4.6$&$4.9$&$5.4$&$10^{-3}\mathrm{K/W}$ \\
%     $R^{\text{iw/ia}}$   &$2.3$&$2.4$&$2.3$&$2.9$&$10^{-4}\mathrm{K/W}$\\
%     $R^{\text{ow/oa}}$   &$1.5$&$0.6$&$0.7$&$0.7$& $10^{-4}\mathrm{K/W}$ \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% the attacker has no motivation to get more

\end{document}
