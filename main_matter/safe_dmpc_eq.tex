\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter[Resilient Primal Decomposition-based dMPC for scarce systems]{Resilient \\Primal Decomposition-based \\distributed \\Model Predictive Control\\ for scarce systems}\label{sec:safe_pddmpc_eq}
\epigraph{\centering So you tell me 'trust me' \\I can trust you\\ Just let me show you \\But I gotta work it out in a shadow of doubt \\'Cause I don't know if I know you}
{\textit{Lie}\\\textsc{Kevin Moore}}

In this chapter, we focus on the analysis of the primal decomposition-based \dmpc{} problem for scarce systems when in the presence of the attack chosen in~\S\ref{sec:anomalous}.

With the knowledge acquired after the analysis, we propose a method to detect the attacks and mitigate its effects.

\minitoc


\section{Scarce systems under attack}\label{sec:analys-scarce-syst}

Different from~\cite{VelardeEtAl2018,MaestreEtAl2021} which propose robust solutions, we propose a resilient \dmpc{} based on a hybrid of analytical/learning active detection method.
For this end, we begin with an analysis of the system under attack.

\subsection{Scarce Systems}\label{sec:scarce-systems}
First we recall the monolithic \mpc{} equivalent problem~\eqref{eq:qp_standard_form}, once more reproduced for the reader's convenience,
\begin{equation}
  \label{eq:qp_standard_form_again}
  \tag{\ref*{eq:qp_standard_form}}
  \begin{aligned}
    \begin{matrix}
      \minimize\limits_{\vec{U}[k]} &
                                                 \frac{1}{2}\norm{\vec{U}[k]}^{2}_{H} + {\vec{f}[k]}^{T}\vec{U}[k] &\\
      \mathrm{subject~ to} &
                             \bar{\Gamma}\vec{U}[k]\preceq {\vec{U}}_{\text{max}}
    \end{matrix}
  \end{aligned},
\end{equation}
and the local problems~\eqref{eq:DOP_local} solved in the primal decomposition, also reproduced,

\begin{equation}
  \label{eq:DOP_local_reprise}
  \tag{\ref*{eq:DOP_local}}
  \bar{J}_{i}^{\star}(\thetaik)=
  \begin{matrix}
    \minimize\limits_{\vec{U}_{i}[k]}&\obji=\frac{1}{2}\norm{\vec{U}_{i}[k]}^{2}_{H_{i}} + {\vec{f}_{i}[k]}^{T}\vec{U}_{i}[k]\\
    \mathrm{subject~ to} & \bar{\Gamma}_{i}\vec{U}_{i}[k] \preceq \thetaik:\lambdaik
  \end{matrix}.
\end{equation}

The unconstrained version of problems~\eqref{eq:qp_standard_form_again} and~\eqref{eq:DOP_local_reprise} are
\begin{align}
  \label{eq:qp_standard_form_unconstrained}
  \begin{aligned}
    \begin{matrix}
      \minimize\limits_{\vec{U}[k]} &
                                                 \frac{1}{2}\norm{\vec{U}[k]}^{2}_{H} + {\vec{f}[k]}^{T}\vec{U}[k] &\\
    \end{matrix},
  \end{aligned}\\
  \label{eq:DOP_local_unconstrained}
  \begin{aligned}
    \begin{matrix}
    \minimize\limits_{\vec{U}_{i}[k]}&\frac{1}{2}\norm{\vec{U}_{i}[k]}^{2}_{H_{i}} + {\vec{f}_{i}[k]}^{T}\vec{U}_{i}[k]\\
    \end{matrix},
  \end{aligned}
\end{align}
and have analytical solutions~\cite{BoydVandenberghe2004}
\begin{align}
  \label{eq:qp_standard_form_unconstrained_solution}
  \vec{U}_{\text{unc}}^{\star}[k]=-H^{-1}\vec{f}[k],\\
  \label{eq:DOP_local_unconstrained_solution}
  \vec{U}_{i_{\text{unc}}}^{\star}[k]=-H_{i}^{-1}\vec{f}_{i}[k].
\end{align}

We call a system scarce when its unconstrained solution $\vec{U}_{\text{unc}}^{\star}[k]$ lies outside the bounds of the polytope formed by the constraints for all $k$, i.e.,
\begin{equation}
\bar{\Gamma}\vec{U}_{\text{unc}}[k]\succ {\vec{U}}_{\text{max}}, \forall k.
\end{equation}

Furthermore, in this chapter, we assume that for all sub-systems, their unconstrained solutions $\vec{U}_{i_{\text{unc}}}^{\star}[k]$ neither respect the local constraints
\begin{equation}
\bar{\Gamma}_{i}\vec{U}_{i_{\text{unc}}}^{\star}[k]\succ \thetaik, \forall i\in\set{M}, \forall k.
\end{equation}

This means that the optimal solution would need more resources (more than $\vec{U}_{\max}$).
So the solution to those \qp{} problems is the projection of the unconstrained solutions onto the polytope. Thus, projecting onto the perimeter of the polytope.

We assume that given projection solves the inequality constrained problem equivalent to an equality constraint problem
\begin{equation}
  \label{eq:qp_standard_form_equality}
  \begin{aligned}
    \begin{matrix}
      \minimize\limits_{\vec{U}[k]} &
                                                 \frac{1}{2}\norm{\vec{U}[k]}^{2}_{H} + {\vec{f}[k]}^{T}\vec{U}[k] &\\
      \mathrm{subject~ to} &
                             \bar{\Gamma}\vec{U}[k]= {\vec{U}}_{\text{max}}
    \end{matrix}
  \end{aligned}.
\end{equation}

\begin{remark}
  Observe that this is not always true using the same $\bar{\Gamma}$ (or $\bar{\Gamma}_{i}$).
  The constraints in~\eqref{eq:qp_standard_form_again} and~\eqref{eq:DOP_local_reprise}, can be interpreted as the intersection of the halfspaces described by the rows of $\bar{\Gamma}$ (or $\bar{\Gamma}_{i}$) and $\vec{U}_{\max}$ (or $\thetaik$), i.e., the halfspaces
  ${\set{S}_i=\setbuild{\vec{x}}{\elem[i,\star]{{\bar{\Gamma}}}\vec{x}\leq\elem[i,\star]{\vec{U}_{\max}}}}$, and the intersection ${\set{S}=\setbuild{\vec{x}}{\bar{\Gamma}\vec{x}\preceq\vec{U}_{\max}}=\bigcap\limits_{i=1}^{\card\vec{U}_{\max}}}\set{S}_{i}$ (and conversely for $\bar{\Gamma}_{i}$ and $\thetaik$).

  For this to be true, only the halfspaces closest to the unconstrained solution are kept and transformed into intersection of hyperplanes (equality constraint).
  This computation of active sets and distances may be costly depending on the number of halfspaces used and the dimensions of the variables.

  For our study cases where $\Gamma$ has all its elements positive (such as postive weighted sum of all inputs), and block diagonal structure with repeating pattern (from the \mpc{} definition~\eqref{eq:construction_Gamma}), we assume we can find the corresponding problem and change $\bar{\Gamma}$ (and $\bar{\Gamma}_i$) accordingly, by removing the proper rows.
\end{remark}

As one can perceive, it leads to a decomposition exactly as in \S\ref{sec:example-interpr}
with local problems~\eqref{eq:example_local_problem} and negotiation equation~\eqref{eq:example_projectedSubgradient_lambda}, reproduced here,
\begin{equation}
  \begin{matrix}
  \label{eq:example_local_problem_reprise}
  \tag{\ref*{eq:example_local_problem}}
    \minimize\limits_{\vec{U}_{i}[k]}&\obji=\frac{1}{2}\norm{\vec{U}_{i}[k]}^{2}_{H_{i}} + {\vec{f}_{i}[k]}^{T}\vec{U}_{i}[k]\\
    \mathrm{subject~ to} & \bar{\Gamma}_{i}\vec{U}_{i}[k] = \thetaik:\lambdaik
  \end{matrix}
\end{equation}
\begin{equation}
  \label{eq:example_projectedSubgradient_lambda_reprise}
  \tag{\ref*{eq:example_projectedSubgradient_lambda}}
 \thetai\pplusone=\thetai\p+\rho\p\left(\lambdai\p-\frac{1}{M}\sum_{j=1}^{M}\vec{\lambda}_j\p\right),\forall i\in\set{M}.
\end{equation}

These are the equations we will use for our analysis.

\subsection{Why scarce systems?}\label{sec:why-scarce-systems}

When a system does not suffer from scarcity, the solution of the unconstrained problem~\eqref{eq:qp_standard_form_unconstrained_solution} respects the inequalities, i.e.,
\begin{equation}
\bar{\Gamma}\vec{U}_{\text{unc}}[k]\preceq {\vec{U}}_{\text{max}},\forall k.
\end{equation}

This way, the elements of $\vec{U}_{\text{unc}}$ lie inside the polytope and none of the constraints is active.
As a consequence, no projection onto the polytope is needed, since it would result in the same point.
That means the solution of the constrained problem is the same as the unconstrained, so the problem is in fact unconstrained.

Instead of needing to solve the local problems~\eqref{eq:DOP_local_reprise} and use a negotiation to decompose the system, the problem doesn't need coordination.
The problem is solved as shown in \S\ref{sec:uncoupled_problems}, i.e., each subsystem solves~\eqref{eq:qp_standard_form_unconstrained_solution} and the solution is already given.

This case is uninteresting because since the agents do not need to compromise to find a consensus, there is no incentive to attack the system to gain more resources since it would not be used and it would not affect the others, who would be nevertheless satisfied.
Furthermore, since no negotiation is needed, there is no communication and the agent would need to find other ways to attack the system.

When the system suffer from scarcity the agents are forced to compete and compromise, what may result in attacks as shown in \S\ref{sec:primal_decomposition} and their dire consequences.

\newcommand{\lagrangianname}{\mathscr{L}}
\newcommand{\lagrangian}{\lagrangianname(\vec{U}_{i}[k],\lambdaik,\thetaik)}
\newcommand{\dualfunctionname}{\mathscr{D}}
\newcommand{\dualfunction}{\dualfunctionname(\lambdaik,\thetaik)}
\newcommand{\equalityfunctionname}{h_i}
\newcommand{\equalityfunction}{\equalityfunctionname(\vec{U}_{i}[k],\thetaik)}
\newcommand{\linearcoefi}{\bar{\Gamma}_{i}H_{i}^{-1}\bar{\Gamma_{i}}^{T}}

\subsection{Analysis of local problems}\label{sec:analysis-local-problems}
Since the optimization problems~\eqref{eq:example_local_problem_reprise} are \qp{} problems with equality constraints, it is known that it has analytical solution~\cite{BoydVandenberghe2004}.
To find this solution we usually use the lagrange duality, so $\lambdaik$ will also have analytical solution in this case.
First, to simplify the notation we define a function $\equalityfunctionname:\R^{\card\vec{U}_{i}[k]}\times \R^{\card\thetaik}$, defined as
\begin{equation}
  \label{eq:lagrangian_equality}
  \equalityfunction=\bar{\Gamma}_{i}\vec{U}_i[k]-\thetaik,
\end{equation}
to put the problem in standard form for equality constraint \qp{} programs:
\begin{equation}
  \begin{matrix}
  \label{eq:local_problem_equality_standard_notation}
    \minimize\limits_{\vec{U}_{i}[k]}&\obji(\vec{U}_{i}[k],\thetaik)\\
    \mathrm{subject~ to} & \equalityfunction= 0:\lambdaik
  \end{matrix}
\end{equation}

With this form, we define the \emph{Lagrangian} function $\lagrangianname : \R^{\card\vec{U}_{i}[k]}\times \R^{\card\lambdaik} \times \R^{\card\thetaik} \to \R$ as
\begin{equation}
  \label{eq:lagrangian_equality}
  \lagrangian=\obji+\lambdaik^T\equalityfunction,
\end{equation}
which embeds the equality constraint into the objective function using $\lambdaik$, called the lagrangian multiplier for equality.
In this chapter, since there is only one type of lagrangian multiplier, we will call it the dual variable.
\begin{remark}
  Observe that by complementary slackness, ${\lambdaikstar}^T\equalityfunctionname(\vec{U}_{i}^{\star}[k],\thetaik)$ will always be zero.
 If any element of $\equalityfunctionname(\vec{U}_{i}^{\star}[k],\thetaik)$ is different from zero, the corresponding element on the dual variable $\lambdaikstar$ will be zero, indicating the constraint is active.
\end{remark}

Then we define the dual function $\dualfunctionname:\R^{\card\lambdaik} \times \R^{\card\thetaik} \to \R$ which is calculated by solving
\begin{equation}
  \label{eq:dual_function_equality}
  \dualfunction=\inf_{\vec{U}_{i}[k]\in\set{F}} \lagrangian,
\end{equation}
where $\set{F}=\dom \equalityfunctionname=\bigcap\limits_{i=1}^{\card\vec{U}_{\max}}\set{P}_{i}$, with $\set{P}_{i}$ being the hyperplanes
${\set{P}_i=\setbuild{\vec{x}}{\elem[i,\star]{{\bar{\Gamma}}}\vec{x}=\elem[i,\star]{\vec{U}_{\max}}}}$.

The solution to $\dualfunction$ is given by the first order \KKT{} optimality condition
\begin{equation}
\nabla_{\vec{U}_{i}[k]}\lagrangian=0
\end{equation}
which results in
\begin{equation}
\vec{U}_{i}[k]=-H_{i}^{-1}(-\bar{\Gamma}_{i}^{T}\lambdai[k]-\vec{f}_{i}[k]).
\end{equation}

Substituting it in $\lagrangian$ yields
\begin{equation}
  \label{eq:dual_function_solution_lambda_theta}
\dualfunction=-\frac{1}{2}\lambdaik^{T}\linearcoefi\lambdaik-\lambdaik^{T}\bar{\Gamma}_{i}H_{i}^{-1}\vec{f}[k]-\lambdaik^{T}\thetaik.
\end{equation}

As presented in~\cite{BoydVandenberghe2004} and as we can see from inspecting~\eqref{eq:dual_function_solution_lambda_theta}, $\dualfunction$ is concave. Thus, we can find $\lambdaikstar$ by solving
\begin{equation}
  \begin{matrix}
    \lambdaikstar=\argmax\limits_{\lambdaik}\maximize\limits_{\lambdaik}&\dualfunction.
  \end{matrix}
\end{equation}

Similarly, using first order \KKT{} condition we make the gradient of $\dualfunction$ vanish
\begin{equation}
\nabla_{\lambdaik}\dualfunction=0,
\end{equation}
which yields
\begin{equation}
  \label{eq:lambda_function_theta}
\lambdaik=-P_{i}\thetaik-\vec{s}_{i}[k],
\end{equation}
where $P_{i}={(\linearcoefi)}^{-1}$ and $\vec{s}_{i}[k]=P_{i}\bar{\Gamma}_{i}H_{i}^{-1}\vec{f}_{i}[k]$.

As we expected, the value of $\lambdaik$ depends on $\thetaik$.
Furthermore, because of the quadratic form, the solution is an affine function.

\begin{remark}
  One can notice, as foreshadowed in \S\ref{sec:attack-interest}, that the solution embeds information not only of the objective function and the reference and state (presence of $H_{i}$ and $\vec{f}_{i}[k]$), but also of the constraints (presence of $\bar{\Gamma}_{i}$ and $\thetaik$).
  So, changes in these parameters affect the resulting value of $\lambdaik$.
\end{remark}

\subsection{Analysis of negotiation}\label{sec:analysis-negotiation}
Taking the negotiation equation~\eqref{eq:example_projectedSubgradient_lambda_reprise},
and substituting the values found in~\eqref{eq:lambda_function_theta} into~\eqref{eq:example_projectedSubgradient_lambda_reprise} yields in
\begin{equation}
  \label{eq:negotiation_equation_substituting}
 \thetai\pplusone=\thetai\p+\rho\p\left((-P_{i}\thetai\p-\vec{s}_{i}[k])-\frac{1}{M}\sum_{j\in\set{M}}(-P_{j}\vec{\theta}_{j}\p-\vec{s}_{j}[k])\right),\forall i\in\set{M}.
\end{equation}

As we can see, we can separate the function into two parts, one which depends on $\thetai$ and other which do not:
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing}
 \thetai\pplusone=\thetai\p+\rho\p\left(-P_{i}\thetai\p+\frac{1}{M}\sum_{j\in\set{M}}P_{j}\vec{\theta}_{j}\p\right)+\rho\p\left(-\vec{s}_{i}[k]+\frac{1}{M}\sum_{j\in\set{M}}\vec{s}_{j}[k]\right),\forall i\in\set{M}.
\end{equation}

We can finally write it in matrix form:
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing}
  \vec{\theta}\pplusone=\mathcal{A}\vec{\theta}\p+\mathcal{\vec{B}}[k]
\end{equation}
where
\begin{equation}
\mathcal{A}=\left[
\begin{matrix}
I-\frac{M-1}{M}\rho\p P_{1} & \frac{1}{M}\rho\p P_{2}&\dots&\frac{1}{M}\rho\p P_{M}\\
\frac{1}{M}\rho\p P_{1}&I-\frac{M-1}{M}\rho\p P_{2}&\dots&\frac{1}{M}\rho\p P_{M}\\
\vdots&\vdots&\ddots&\vdots\\
\frac{1}{M}\rho\p P_{1}&\frac{1}{M}\rho\p P_{2}&\dots&I-\frac{M-1}{M}\rho\p P_{M}\\
\end{matrix}
\right]
\end{equation}
\begin{equation}
\mathcal{\vec{B}}[k]=\left[
\begin{matrix}
-\frac{M-1}{M}\rho\p \vec{s}_{1}[k]+\frac{1}{M}\rho\p \vec{s}_{2}[k]\dots-\frac{1}{M}\rho\p \vec{s}_{M}[k]\\
\frac{1}{M}\rho\p \vec{s}_{1}[k]-\frac{M-1}{M}\rho\p \vec{s}_{2}[k] \dots-\frac{1}{M}\rho\p \vec{s}_{M}[k]\\
\vdots\\
\frac{1}{M}\rho\p \vec{s}_{1}[k]+\frac{1}{M}\rho\p \vec{s}_{2}[k]\dots-\frac{M-1}{M}\rho\p \vec{s}_{M}[k]\\
\end{matrix}
\right]
\end{equation}
which is a \dt{} system with forced input ($\mathcal{\vec{B}}$).
The system varies with time, since $p$ changes, but it vanishes as it approaches infinity, usually with~\eqref{eq:rho_update}.
So, we can use Lyapunov criteria~\cite[\S8.6]{Hespanha2009}, i.e., the eigenvalues of $\mathcal{A}$, must be inside the unit circle for the system to be stable.
For smaller systems, we can use Jury's criteria to test the system~\cite{Jury1962}.
\begin{remark}
  Observe that there is a linear dependence between the rows of $\mathcal{A}$ embeded on the negotiation due to the equality constraint.
  So, some eigenvalues (more precisely $\card\thetai$) are bound to be equal to $1$ (sum of all rows equals to 1).
  \begin{equation}
    \label{eq:1}
    \1^{T}\mathcal{A}=\1^{T}
  \end{equation}
  So to calculate the eigenvalues we need to remove accordingly one of the $\theta_{i}$ (rows and columns).
\end{remark}

As we can see, the values of the eigenvalues depend only on the values of $P_{i}$ and $\rho\p$.
Since the $P_{i}$ are system parameters, the designer of the control can only tune $\rho\p$.

\begin{remark}
  Observe that as $p$ increases, $\rho\p$ vanishes, and as consequence the matrix $\mathcal{A}$ presents less and less influence of $P_{i}$ and its eigenvalues approach $1$.
\end{remark}

Once $\rho\p$ is well tuned for the corresponding system, the matrix $\mathcal{A}$ evolves always in the same way.
So if there is a change in this evolution, it means an the system presents an anomalous behavior.

If we suppose the system is attacked, any attack in $P_{i}$ (objective function or constraints) changes the system's dynamic (change in $\mathcal{A}$), and may destabilize it if drives the eigenvalues outside the unit circle, as shown in \S\ref{sec:attack-interest}.
On the other hand, attacks in $\vec{s}_{i}[k]$ (objective function, constraints or state/reference) may change the steady state value (change in $\mathcal{\vec{B}}[k]$).

If we take the attack model~\ref{eq:selfish_attack}, reproduced here,
\begin{equation}
  \label{eq:linear_attack_reprise}
  \tag{\ref*{eq:linear_attack}}
  \tilde{\vec{\lambda}}_{i}[k]=T_{i}[k]\lambdaik,
\end{equation}
modify adequately~\eqref{eq:lambda_function_theta}, and substitute into~\eqref{eq:example_projectedSubgradient_lambda_reprise}, we can have the dynamics of an attacked system
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing_tilde}
  \vec{\theta}\pplusone=\tilde{\mathcal{A}}[k]\vec{\theta}\p+\tilde{\mathcal{\vec{B}}}[k]
\end{equation}
where
\begin{equation}
\tilde{\mathcal{A}}[k]=\left[
\begin{matrix}
I-\frac{M-1}{M}\rho\p T_{1}[k]P_{1} & \frac{1}{M}\rho\p T_{2}[k]P_{2}&\dots&\frac{1}{M}\rho\p T_{M}[k]P_{M}\\
\frac{1}{M}\rho\p T_{1}[k]P_{1}&I-\frac{M-1}{M}\rho\p T_{2}[k]P_{2}&\dots&\frac{1}{M}\rho\p T_{M}[k]P_{M}\\
\vdots&\vdots&\ddots&\vdots\\
\frac{1}{M}\rho\p T_{1}[k]P_{1}&\frac{1}{M}\rho\p T_{2}[k]P_{2}&\dots&I-\frac{M-1}{M}\rho\p T_{M}[k]P_{M}\\
\end{matrix}
\right]
\end{equation}
\begin{equation}
\tilde{\mathcal{\vec{B}}}[k]=\left[
\begin{matrix}
-\frac{M-1}{M}\rho\p T_{1}[k]\vec{s}_{1}[k]+\frac{1}{M}\rho\p T_{2}[k]\vec{s}_{2}[k]\dots-\frac{1}{M}\rho\p T_{M}[k]\vec{s}_{M}[k]\\
\frac{1}{M}\rho\p T_{1}[k]\vec{s}_{1}[k]-\frac{M-1}{M}\rho\p T_{2}[k]\vec{s}_{2}[k] \dots-\frac{1}{M}\rho\p T_{M}[k]\vec{s}_{M}[k]\\
\vdots\\
\frac{1}{M}\rho\p T_{1}[k]\vec{s}_{1}[k]+\frac{1}{M}\rho\p T_{2}[k]\vec{s}_{2}[k]\dots-\frac{M-1}{M}\rho\p T_{M}[k]\vec{s}_{M}[k]\\
\end{matrix}
\right]
\end{equation}

And as we have attested, the cheating matrices $T_{i}[k]$ can change the eigenvalues of the matrix, making the system oscillate and not converge. It can also be adjusted to only drive the negotiation to a new point, modifying $\mathcal{B}$, but as we see it modifies at the same time the dynamics. This way, the attacker may compromise between its greediness and not breaking the system, what is worse for every agent, the attacker included.

As one can notice, we could create an anomaly detector by supervising the matrix $\mathcal{A}$.
Once there is a change in some of the variables (or eigenvalues) the attack is detected.

Unfortunately, this method gives us only a detection, but not necessarily from which agent the attack comes, what could useful for a mitigation algorithm.
Besides, if we were to estimate all elements of $\mathcal{A}$, we would need to estimate $M^{2}{(\card\thetai)}^{2}$ elements\footnote{$M^{2}$ blocks of size ${\card\thetai}$}.
Exploiting the known structures of the matrix and their blocks, we could reduce to at least $2M{(\frac{\card\thetai+{\card\thetai}^{2}}{2})}$ elements\footnote{($M$ diagonal + $M$ off diagonal) symmetric blocks of size ${\card\thetai}$ (only upper triangle)}.

In the next section, we derive a detection mechanism, also based on estimation, which can not only detect the attacks, but also isolate the perpetrators, estimating only half of the elements cited.

\section{A detection mechanism}\label{sec:detection-mechanism}

Taking~\eqref{eq:linear_attack_reprise}
\begin{equation}
  \label{eq:lambda_function_theta_tilde}
\tilde{\vec{\lambda}}_{i}[k]=-\tilde{P}_{i}\thetaik-\tilde{\vec{s}}_{i}[k],
\end{equation}
where


% \begin{figure}[h]
%   \centering
%   \begin{subfigure}{.45\textwidth}
%     \includegraphics[width=\textwidth]{../img/original-minimum.png}
%     \caption{Original minimum.}
%     \label{fig:first}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.45\textwidth}
%     \includegraphics[width=\textwidth]{../img/new-minimum-selfish.png}
%     \caption{Minimum after attack.}
%     \label{fig:second}
%   \end{subfigure}
%   \caption{Effects of non-conforming behaviors on optimal value. \todo{Refaire les images}}
%   \label{fig:figures}
% \end{figure}

% \begin{figure}[h]
%   \centering
%   \begin{subfigure}{0.45\textwidth}
%     \includegraphics[width=\textwidth]{../img/ignoreX.png}
%     \caption{Optimal value after ignoring attacker.}
%     \label{fig:third}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.45\textwidth}
%     \includegraphics[width=\textwidth]{../img/correctX.png}
%     \caption{Optimal value after trying to recover original behavior.}
%     \label{fig:third}
%   \end{subfigure}

%   \caption{Recovery options.}\label{fig:figures}
% \end{figure}

% \section{Study case}
% \label{sec:study-case}

% Model 3R-2C \cite{GoudaEtAl2002}
% \begin{figure}[h]
%   \centering
%   \begin{circuitikz}[european]
%     \draw (0,0) node[tlground]{} to[isource, l=$P^{\text{heat}}$] ++(0,2) to[short, -*] ++(1.5,0) coordinate (a);

%     \draw (a) node[above]{$T^{\text{in}}$}  to[C=$C^{\text{air}}$] ++(0,-2) node[tlground]{};
%     \draw (0,-3) node[tlground]{} to[isource, l=$I^{\text{sol}}$] ++(0,2)
%     to[short, -*] ++(1.5,0) coordinate (b);
%     \draw (b) to[C=$C^{\text{walls}}$] ++(0,-2) node[tlground]{};

%     \draw (a) -- ++(2,0) coordinate (c) -- ++(0,-.5) to[R=$R^{\text{iw/ia}}$] ++(0,-2) -- ++(0,-.5) coordinate (d);

%     \draw (b) node[above]{$T^{\text{walls}}$} to[short,-*] (d);

%     \draw (c) --  ++(2.5,0) -- ++(0,-.5) to[R=$R^{\text{oa/ia}}$] ++(0,-2) -- ++(0,-.5) coordinate (e);

%     \draw (d) to[R=$R^{\text{ow/oa}}$] (e) to[battery,l=$T^{\text{out}}$] ++(0,-2) node[tlground]{};
%   \end{circuitikz}
%   \caption{Thermic Model 3R-2C of a room.}
%   \label{fig:3R2C_model}
% \end{figure}

% The state-space model of each subsystem is given by:
% \begin{equation}
%   \begin{matrix}
%     \label{eq:systems_cont}
%     \dot{\vec{x}}_{i}(t)  &=&{A_{c}}_{i}\vec{x}_{i}(t) &+& {B_{c}}_{i}\vec{u}_{i}(t)\\
%     \vec{y}_{i}(t)        &=&{C_{c}}_{i}\vec{x}_{i}(t) &&
%   \end{matrix},
% \end{equation}
% where
% \begin{equation}
%   \label{eq:4}
%   \begin{matrix}
%     A_{\mathrm{c}_{i}}=\left[
%     \begin{matrix}
%       -\frac{1}{C^{\text{walls}}_{i}R^{\text{oa/ia}}_{i}}-\frac{1}{C^{\text{walls}}_{i}R^{\text{iw/ia}}_{i}}& \frac{1}{C^{\text{walls}}_{i}R^{\text{iw/ia}}_{i}}\\
%       \frac{1}{C^{\text{air}}_{i}R^{\text{iw/ia}}_{i}} &-\frac{1}{C^{\text{air}}_{i}R^{\text{ow/oa}}o_{i}}-\frac{1}{C^{\text{air}}_{i}R^{\text{iw/ia}}_{i}}
%     \end{matrix}\right]\\
%     \begin{matrix}
%       B_{\mathrm{c}_{i}}=\left[
%       \begin{matrix}  \frac{10}{C^{\text{walls}}_{i}}& 0\end{matrix}
%                                                        \right]\T&C_{\mathrm{c}_{i}}=\left[\begin{matrix}1 & 0\end{matrix}\right]
%     \end{matrix}
%   \end{matrix}
% \end{equation}
% where ${\vec{x}_{i}=[{{x}_{A}}_{i}\T\ {{x}_{W}}_{i}\T]\T}$. ${x_A}_i$ and ${x_W}_i$ are the mean temperatures of the air and walls inside room~$i$. $\vec{u}_{i}$ is the input (the heating power)
% for the corresponding room. The inputs are constraint by ${\sum_{i=1}^{4}\vec{u}_{i}(t)\preceq 4\mathrm{kW}}$.

% \begin{table}[b]
%   \centering
%   \caption{Model Parameters}\label{tab:modelParamMeaning}
%   \begin{tabular}[b]{cl}
%     \toprule
%     Symbol&Meaning\\
%     \midrule
%     $C^{\text{air}}_{i}$  &Heat Capacity of Inside Air\\
%     $C^{\text{walls}}_{i}$ &Heat Capacity of External Walls\\
%     $R^{\text{iw/ia}}_{i}$ &Resist. Between Inside Air and Inside Walls\\
%     $R^{\text{ow/oa}}_{i}$ &Resist. Between Outside Air and Outside Walls\\
%     $R^{\text{oa/ia}}_{i}$ &Resist. Between Inside and Out.\ Air (from windows)\\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \begin{table}[b]
%   \centering
%   \caption{
%     Parameters for each agent}\label{tab:modelParam}
%   \begin{tabular}[t]{cccccc} \toprule
%     Symbol& I & II & III & IV &Unit\\
%     \midrule
%     $C^{\text{walls}}$   &$5.4$&$4.9$&$4.7$&$4.7$ &$10^{4}\mathrm{J/K}$ \\
%     $C^{\text{air}}$     &$7.5$ &$8.4 $&$8.2$ &$7.7$&$10^{4}\mathrm{J/K}$  \\
%     $R^{\text{oa/ia}}$   &$5.2$&$4.6$&$4.9$&$5.4$&$10^{-3}\mathrm{K/W}$ \\
%     $R^{\text{iw/ia}}$   &$2.3$&$2.4$&$2.3$&$2.9$&$10^{-4}\mathrm{K/W}$\\
%     $R^{\text{ow/oa}}$   &$1.5$&$0.6$&$0.7$&$0.7$& $10^{-4}\mathrm{K/W}$ \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% the attacker has no motivation to get more

\end{document}
