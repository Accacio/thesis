\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter[Resilient Primal Decomposition-based dMPC]{Resilient \\Primal Decomposition-based \\distributed \\Model Predictive Control}\label{sec:safe_pddmpc_ineq}
\epigraph{\centering Desperate times call for desperate measures}
{\textit{Title}\\\textsc{Author}}

In this chapter, we carry on the work from the last chapter relaxing some assumptions to make the problem to be solved more generic.
We apply the primal decomposition to the problem, and then, we compare the cases when the system presents nominal behavior and when the system is under the proposed attack model.

From the analysis, we notice how the relaxation of the assumption increases drastically the complexity of the problem.

Employing some assumptions, we take a powerful tool of our identification utility belt, and use it to create an approach to circumvent the complexity of the system, allowing us to adapt the detection and mitigation presented in the last chapter.

\minitoc

\section{Relaxing the problem}\label{sec:not-so-scarce}
We begin again with the monolithic \mpc{} equivalent problem~\eqref{eq:qp_standard_form}.
We reproduce yet another time the problem, but with a twist, now the problem also has a decomposable constraint set $\set{U}$:
\begin{equation}
  \label{eq:qp_standard_form_again_set_constraint}
  \begin{aligned}
    \begin{matrix}
      \minimize\limits_{\vec{U}[k]} &
                                                 \frac{1}{2}\norm{\vec{U}[k]}^{2}_{H} + {\vec{f}[k]}^{T}\vec{U}[k] &\\
      \mathrm{subject~ to} & \bar{\Gamma}\vec{U}[k]\preceq {\vec{U}}_{\text{max}} \\
                                    & \vec{U}[k]\in \set{U}
    \end{matrix}
  \end{aligned}.
\end{equation}
We decompose the system using the primal decomposition, and the local problems also have constraint sets, denoted $\set{U}_{i}$:
\begin{equation}
  \label{eq:DOP_local_set_constraint}
  \eqoptobji(\thetaik)[k]=
  \begin{matrix}
    \minimize\limits_{\vec{U}_{i}[k]}&\frac{1}{2}\norm{\vec{U}_{i}[k]}^{2}_{H_{i}} + {\vec{f}_{i}[k]}^{T}\vec{U}_{i}[k]\\
    \mathrm{subject~ to} & \bar{\Gamma}_{i}\vec{U}_{i}[k] \preceq \thetaik:\lambdaik\\
    & \vec{U}_{i}[k]\in \set{U}_{i}
  \end{matrix},
\end{equation}
and we use the projected subgradient method~\eqref{eq:projectedSubgradient_lambda} (reproduced for convenience) to solve the main problem:
\begin{equation}
  \label{eq:projectedSubgradient_lambda_reprise}
  \tag{\ref*{eq:projectedSubgradient_lambda}}
  \vec{\theta}[k]\pplusone=\Proj^{\set{S}}(\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p),
\end{equation}
but with ${\set{S} = \setbuild{\vec{\theta}[k]}{I_{c}^{M}\vec{\theta}[k]\preceq \vec{U}_{\max}}}$.
\begin{remark}
  As seen in~\cite{BoydEtAl2015}, if we have separable constraints sets, such as the $\set{U}_{i}$ in our problem, we can modify the local objectives $\obji$ so \begin{equation*}
    J_{i}(\optthetai)=\infty\text{, if }\vec{U}_{i}^{\star}\notin\dom{\obji}.
  \end{equation*}
  Since this change modifies the objective functions, it also modifies the corresponding gradients, and $\lambdai$ as consequence.
  So, this change guides the coordinator to choose $\thetai$ which respect $\dom{\obji}$.
\end{remark}

In this chapter, we still assume the original constraint~\eqref{eq:linear_constraint} to have at most as many rows as columns, i.e., $\card{\vec{u}_{\max}}\leq n_{u}$.
However, we relax the assumption made in \S\ref{sec:scarce-systems}.
The systems are not necessarily scarce anymore, i.e., the unconstrained solutions
$\optuncU=-H^{-1}\vec{f}[k]$ and $\optuncUik=-H_{i}^{-1}\vec{f}_{i}[k]$ may or may not respect the constraints.

As we will see in next section, despite the relaxation of this assumption, making the problem more generic, it causes the solution of the local problems exponentially more complex.

\todo{Nonetheless, to assure some properties we make another assumption, we assume the unconstrained solutions respect the constraint sets,\note{where to put this?? Maybe when discussing artificial scarcity}
\begin{align}
  \optuncU\in\set{U}\\
  \optuncUik\in\set{U}_{i}.
\end{align}
}

\section{Impact on local problems' solution}\label{sec:impact-local-problem}
We can repeat the analysis made in the last chapter to see how the change on assumption influences the solutions of the problem.

Although now we have \qp{} problems with inequality constraints, we still can use the same method to find a analytical solution.

We similarly define a function $\inequalityfunctionname:\R^{\card\vec{U}_{i}[k]}\times \R^{\card\thetaik}$:
\begin{equation}
  \label{eq:lagrangian_inequality}
  \inequalityfunction=\bar{\Gamma}_{i}\vec{U}_i[k]-\thetaik,
\end{equation}
and use it to put the problem in standard form for inequality constraint \qp{} programs:
\begin{equation}
  \begin{matrix}
  \label{eq:local_problem_equality_standard_notation}
    \minimize\limits_{\vec{U}_{i}[k]}&\obji(\vec{U}_{i}[k],\thetaik)\\
    \mathrm{subject~ to} & \inequalityfunction\preceq 0:\lambdaik
  \end{matrix},
\end{equation}
with variables $\lambdaik$ associated to the constraints.
\begin{remark}
  Usually we make the distinction between the dual variables associated to the inequality and equality constraints by using different symbols.
  However, as in the problems we have only one kind, we use the same symbol and by context the reader can distinguish to which kind the symbol corresponds.
\end{remark}

We can again define the \emph{Lagrangian} function $\lagrangianname : \R^{\card\vec{U}_{i}[k]}\times \R^{\card\lambdaik} \times \R^{\card\thetaik} \to \R$ as
\begin{equation}
  \label{eq:lagrangian_equality}
  \lagrangian=\obji(\vec{U}_{i}[k],\thetaik)+\lambdaik^T\inequalityfunction,
\end{equation}
which embeds the inequality constraint into the objective function using $\lambdaik$, in this case the lagrangian multiplier for \emph{inequality}.
Again, since there is only one type of lagrangian multiplier, we will call it simply as the dual variable.

\begin{remark}
  The complementary slackness still holds, ${\lambdaikstar}^T\inequalityfunctionname(\vec{U}_{i}^{\star}[k],\thetaik)$ will always be zero.
  If any element of $\inequalityfunctionname(\vec{U}_{i}^{\star}[k],\thetaik)$ is smaller than zero, the corresponding element on the dual variable $\lambdaikstar$ will be zero, indicating the constraint is active.
\end{remark}

We can define the dual function $\dualfunctionname:\R^{\card\lambdaik} \times \R^{\card\thetaik} \to \R$ also calculated by solving
\begin{equation}
  \label{eq:dual_function_equality}
  \dualfunction=\inf_{\vec{U}_{i}[k]\in\set{F}} \lagrangian,
\end{equation}
but now where $\set{F}=\dom \inequalityfunctionname=\bigcap\limits_{i=1}^{\card\vec{U}_{\max}}\set{H}_{i}$, with $\set{H}_{i}$ being the halfspaces
${\set{H}_i=\setbuild{\vec{x}}{\elem[i,\star]{{\bar{\Gamma}}}\vec{x}\leq\elem[i,\star]{\vec{U}_{\max}}}}$.
Using the first order \KKT{} optimality condition we have
\begin{equation}
\nabla_{\vec{U}_{i}[k]}\lagrangian=0
\end{equation}
yielding as well
\begin{equation}
\vec{U}_{i}[k]=-H_{i}^{-1}\vec{f}_{i}[k]+H_{i}^{-1}\bar{\Gamma}_{i}^{T}\lambdaik,
\end{equation}
which we know is
\begin{equation}
\vec{U}_{i}[k]=\optuncUik-H_{i}^{-1}\bar{\Gamma}_{i}^{T}\lambdaik.
\end{equation}

Substituting it in $\lagrangian$ results
\begin{equation}
  \label{eq:dual_function_solution_lambda_theta_ineq}
\dualfunction=-\frac{1}{2}\lambdaik^{T}\linearcoefi\lambdaik-\lambdaik^{T}\bar{\Gamma}_{i}H_{i}^{-1}\vec{f}[k]-\lambdaik^{T}\thetaik,
\end{equation}
which is also concave.

Then, similarly we can find $\lambdaikstar$ by solving an optimization problem.
However, since we are using inequalities, we can only have $\lambdaik$ positive~\cite{BoydVandenberghe2004}:
\begin{equation}
  \label{eq:ineq_lambda_equation}
  \begin{matrix}
    \lambdaikstar=\argmax\limits_{\lambdaik}\left\{
    \begin{matrix}
    \maximize\limits_{\lambdaik}&\dualfunction\\
      \mathrm{subject~ to}& \lambdaik\preceq\0
    \end{matrix}\right\}.
  \end{matrix}
\end{equation}

As we know, by complementary slackness, the value of the elements of $\lambdaik$ being $0$ (or different of $0$) will depend on the status of the corresponding constraint for the unconstrained solution $\optuncUik$.
So the complete value of $\lambdaik$ will depend of the permutation of the status of the inequality constraint.
Depending on the form of the constraints, if we have $\nineq$ constraints, we may have potentially $2^{\nineq}$ permutations.
\begin{remark}
  Here we say potentially because not necessarily all combinations are possible.
  In \S\ref{sec:cons-about-stat} we discuss briefly about the number of permutations and show some cases where the number of permutations is not equal to $2^{\nineq}$.
\end{remark}

To solve~\eqref{eq:ineq_lambda_equation}, we can use the fact that the status of the constraints depends on the value of $\thetaik$.
This way, we can notice that the set of constraints divides the space of $\thetaik$ into multiple regions, depending if the constraints are active or not for $\optuncUik$.

$\optuncUik$
$\optuncU$

Imagine an example where we have $2$ constraints with associated $\elem[1]{\lambdai}$ and $\elem[2]{\lambdai}$, the $\thetai$ solution space is partitioned like in Fig.~\ref{fig:constraints_partition_theta} (indices $[k]$ suppressed for brevity).
\begin{figure}[h]
\centering
\begin{tikzpicture}[every node/.style={align=center}]
\def\height{3}
\def\length{5}
\node at (0,0) {} coordinate (p0)  ++(0:\length*2.4/5) coordinate (p1) ;
\node at (0,0) {} ++(90:\height*2.7/5) coordinate (p2) ;

\draw[black] (p2) ++(-1:5) coordinate (p3);
\draw[black] (p1) ++(80:3) coordinate (p4);
\coordinate (p6) at (p3|-p4);

\coordinate (p5) at (intersection cs:first line={(p2)--(p3)}, second line={(p1)--(p4)});
\path[fill=gray!20] (p6) -- (p4) -- (p5) -- (p3);
\draw[black] (p2) -- (p3);
\draw[black] (p1) -- (p4);

\draw[-latex] (0,-.1) - - (0,\height) node[left]{$\elem[2]{\thetai}$};
\draw[-latex] (-.1,0) - - (\length,0) node[below]{$\elem[1]{\thetai}$};

\node at (\length*1.2/5,\height*1.25/5) {$\elem[1]{\lambdai}\neq0$\\$\elem[2]{\lambdai}\neq0$};
\node at (\length*1.2/5,\height*3.8/5) {$\elem[1]{\lambdai}=0$\\$\elem[2]{\lambdai}\neq0$};
\node at (\length*4/5,\height*1.25/5) {$\elem[1]{\lambdai}\neq0$\\$\elem[2]{\lambdai}=0$};
\node at (\length*4/5,\height*3.8/5) {$\elem[1]{\lambdai}=0$\\$\elem[2]{\lambdai}=0$};
\end{tikzpicture}
\caption{Two constraints partitioning $\thetai$ solution space.}\label{fig:constraints_partition_theta}
\end{figure}
For any $\thetai$ inside the grey area, the solution to~\eqref{eq:local_problem_equality_standard_notation} will be unconstrained, i.e., the constraint will be active.
Thus, for this case, by definition the value of $\lambdai=\0$.
For all other cases, where we know at least one constraint in~\eqref{eq:local_problem_equality_standard_notation} is inactive, we can remove the constraint in~\eqref{eq:ineq_lambda_equation} for the element of $\lambdai$ corresponding to the inactive constraint.
For instance, if all constraints in~\eqref{eq:local_problem_equality_standard_notation} are inactive, we can remove all constraints of~\eqref{eq:ineq_lambda_equation}, which will result in problem~\eqref{eq:eq_lambda_equation}, whose solution we known already:
\begin{equation}
  \label{eq:lambda_function_theta_reprise}
  \tag{\ref*{eq:lambda_function_theta}}
  \lambdaik=-\Plin\thetaik-\sik,
\end{equation}
\newcommand{\Plinineqnonzero}[1][\star]{\overset{#1}{\Plin}}
\newcommand{\sikineqnonzero}[1][\star]{\overset{#1}{\vec{s}_{i}}[k]}

By removing a set of constraints, we can solve a similar problem, but with reduced number of elements in $\thetai$ and $\lambdai$.
For example, let us suppose the first constraint in~\eqref{eq:local_problem_equality_standard_notation} is active.
We make $\elem[1]{\lambdai}=0$ and set it aside, we remove the corresponding lines of $\bar{\Gamma}_{i}$ and $\thetai$, and then we solve for all other elements of $\lambdai$, removing the remaining constraints in~\eqref{eq:ineq_lambda_equation}. This results in
\begin{equation}
  \label{eq:lambda_function_theta_some_active}
  \elem[2:\elemend]{\lambdai}[k]=-\Plinineqnonzero[2:\card{\lambdai}] \elem[2:\elemend]{\thetai}[k]-\sikineqnonzero[2:\card{\lambdai}],
\end{equation}
where $\Plinineqnonzero[2:\card{\lambdai}]={(\linearcoefiineqnonzero[2:\elemend,\star])}^{-1}$ and $\sikineqnonzero[2:\card{\lambdai}]=\Plinineqnonzero[2:\card{\lambdai}]\elem[2:\elemend,\star]{\bar{\Gamma}_{i}}H_{i}^{-1}\vec{f}_{i}[k]$.

If we do this for all partitions, i.e., all possible permutations of active and inactive constraints, we will have as many equations similar to~\eqref{eq:lambda_function_theta_some_active} as permutations.
This result is the base for calculating the solution of the original problem~\eqref{eq:local_problem_equality_standard_notation} as shown in~\cite{BemporadEtAl2002,AlessioBemporad2009}, which is the base for explicit \mpc{}.

However, the equations have different sizes of elements (as many elements as inactive constraints).
To make all equations same-sized, we recover the elements of $\lambdai$ set to zero and set aside.
If we take the same example, Eq.~\eqref{eq:lambda_function_theta_some_active} would become
\begin{equation}
  \label{eq:lambda_function_theta_some_active_same_size}
  \lambdaik=
  \left[
    \begin{matrix}
      0\\
      \elem[2:\elemend]{\lambdai}[k]
    \end{matrix}
  \right]
  =
  -
  \left[
    \begin{matrix}
      0&0\\
      0&\Plinineqnonzero[2:\card{\lambdai}]
    \end{matrix}
  \right]
  \thetaik
  -
  \left[
    \begin{matrix}
      0\\
      \sikineqnonzero[2:\card{\lambdai}]
    \end{matrix}
  \right]
\end{equation}


To facilitate the notation, let us use a binary representation such if we have $\nineq$, we use $\nineq$ binary digits to represent the active constraints.
And we will mark the digtis $1$ if the constraint is active and $0$ if inactive, indicating which constraints were removed or kept.

If we retake the $2$-constraints example in Fig.~\ref{fig:constraints_partition_theta}, ${00}_{(2)}$ would represent both constraints inactive (bottom left quadrant), ${01}_{(2)}$ and ${10}_{(2)}$ represents one of the constraints are active (bottom right and top left quadrants, respectively) and ${11}_{(2)}$ where both constraints are active (grey area in top right quadrant).
We can then use the base 10 to represent such partitions.
This way~\eqref{eq:lambda_function_theta_some_active_same_size} can be written as
\begin{equation}
  \label{eq:lambda_function_theta_same_size_coded}
  \lambdaik=-\Plin^{(2^{n_{\text{ineq}-1}})}\thetaik-\sik^{(2^{n_{\text{ineq}-1}})},
\end{equation}
with index $(2^{n_{\text{ineq}-1}})$ indicating that only the first constraint is active.

Then, we can write the complete solution of~\eqref{eq:ineq_lambda_equation} as
a \pwa{} function
\begin{equation}
  \label{eq:lambdafuntheta}
  \begin{aligned}
    \lambdaik=
    \begin{cases}
      -\Plinineq\thetaik-\sikineq,&\text{if}\ G^{0}[k]\thetaik \preceq b^{0}[k] \\
      \qquad\quad \vdots&\qquad\quad \vdots\\
      -\Plinineq[i][2^{\nineq}-1]\thetaik-\sikineq[i][2^{\nineq}-1],&\text{if}\ G^{2^{\nineq}-1}[k]\thetaik \preceq b^{2^{\nineq}-1}[k] \\
    \end{cases}
  \end{aligned}
\end{equation}
with halfspaces defined by the pairs $(G^{n}[k],\vec{b}^{n}[k])$ representing the partitions of the solution space which may vary with time $k$.
\begin{remark}
  Determining the exact halfspaces is not in the scope of this work.
\end{remark}

Here we can begin to regard the change caused by the relaxation, whereas we had only one equation relating $\lambdai$ to $\thetai$, when the system was scarce, now we have potentially $2^{\nineq}$ different equations.

\begin{remark}\label{rem:sparse_solutions}
  As one may expect, by definition, the $\Plinineq[i][n]$ and $\sikineq[i][n]$ are increasingly sparse, as $n$ increases more constraints are active and more blocks are equal to zero, culminating in $\Plinineq[i][2^{\nineq}-1]=0_{\card{\lambdai}}$ and $\sikineq[i][2^{\nineq}-1]=\0_{\card{\lambdai}}$.
\end{remark}
\subsection{Considerations about status of constraints}\label{sec:cons-about-stat}
It is important to observe that some there are some sets of constraints in which the permutation is not necessarily $2^{\nineq}$.

For example, in some cases there is no partition where all constraints are active, i.e., the intersection of halfspaces is empty (see Fig.~\ref{fig:non_feasible}).
Problems with such constraints are infeasible, thus we ignore them in this work.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \def\ang{-35}
    \def\vecmag{.25}
    \node at (0,0) {} coordinate(p1) ++(\ang:2.5) coordinate(p2) ++(\ang-90:1.5) coordinate(p3) ++(\ang-180:2.5) coordinate(p4);

    \path (p1) -- (p2) coordinate[pos=-0.2](a1l) coordinate[pos=1.2](a1r) coordinate[pos=0.8](m12);
    \draw[-latex] (a1l) -- (m12) -- ([turn] -90:0.5) node[left]{$\vec{\eta}_1$};
    \draw  (m12) -- (a1r);

    \path (p3) -- (p4) coordinate[pos=-0.2](a3l) coordinate[pos=1.2](a3r) coordinate[pos=0.8](m34);
    \draw[-latex] (a3l) -- (m34) -- ([turn] -90:0.5) node[right]{$\vec{\eta}_2$};
    \draw  (m34) -- (a3r);

  \end{tikzpicture}
  \caption{Set of constraints with no intersection.}\label{fig:non_feasible}
\end{figure}

In other cases, there is no combination where all constraints are inactive.
We give some different examples.
For instance, for constraints with normals $\vec{\vec{\eta}}_{i}$, if the angles of adjacent halfspaces $\vecangle{\vec{\eta}_{i}}{\vec{\eta}_{j}}$ is $180^{o}$ and the intersection is not nil (see Fig.~\ref{fig:parallel_only_one_active}), one of the constraints will always be active.
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \def\ang{25}
    \def\vecmag{.5}
    \node at (0,0) {} coordinate(p1) ++(\ang:2.5) coordinate(p2) ++(\ang-90:1.) coordinate(p3) ++(\ang-180:2.5) coordinate(p4);
    \path[fill=gray!20] (p1) -- (p2) -- (p3) -- (p4);
    \node at (barycentric cs:p1=1,p2=1,p3=1,p4=1) {$\mathcal{P}$};

    \path (p1) -- (p2) coordinate[pos=-0.2](a1l) coordinate[pos=1.2](a1r) coordinate[pos=0.5](m12);
    \draw[-latex] (a1l) -- (m12) -- ([turn] 90:0.5) node[right]{$\vec{\eta}_1$};
    \draw  (m12) -- (a1r);

    \path (p3) -- (p4) coordinate[pos=-0.2](a3l) coordinate[pos=1.2](a3r) coordinate[pos=0.5](m34);
    \draw[-latex] (a3l) -- (m34) -- ([turn] 90:0.5) node[right]{$\vec{\eta}_2$};
    \draw  (m34) -- (a3r);
  \end{tikzpicture}
  \caption{Two constraints with $\vecangle{\vec{\eta}_{1}}{\vec{\eta}_{2}}=180^{o}$.}\label{fig:parallel_only_one_active}
\end{figure}

Yet another example, are when the constraints form a convex polyhedron.
We present a minimal example of polyhedron in Fig.~\ref{fig:triangle_inequality}, a triangle in $\R^{2}$ formed by $3$ inequality constraints.
In this case, there will always be at least $2$ active constraints.
\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \node at (0,0) {} coordinate(p1) ++(15:2.5) coordinate(p2) ++(-45:2.5) coordinate(p3) ++ (-195:4.0) coordinate(p4);
    \path[fill=gray!20] (p1) -- (p2) -- (p3);
    \node at (barycentric cs:p1=1,p2=1,p3=1) {$\mathcal{P}$};

    \foreach \X [count=\Y] in {2,...,4}
    {
      \path (p\Y) -- (p\X) coordinate[pos=-0.2](a\Y{}l) coordinate[pos=1.2](a\Y{}r) coordinate[pos=0.5](m\Y\X);
      \draw[-latex] (a\Y{}l) -- (m\Y\X) -- ([turn]90:.5) node[right]{$\vec{\eta}_\Y$};
      \draw (m\Y\X) -- (a\Y{}r);
    }
  \end{tikzpicture}
  \caption{A $3$-sided polyhedron.}\label{fig:triangle_inequality}
\end{figure}

Since we suppose there are at most the number of constraints as dimensions, it is not possible to create polyhedrons.

\section{Impact on negotiation equations}\label{sec:impact-local-problem}
We can try to apply the same logic used in \S\ref{sec:analysis-negotiation}, by substituting~\eqref{eq:lambdafuntheta} into~\eqref{eq:projectedSubgradient_lambda_reprise} and vice-versa, and compare with the results in \S\ref{sec:analysis-negotiation}.
In this attempt, we can fully contemplate how the relaxation of the assumptions increases the problem's complexity.

The first difficulty is already on the projected subgradient method, which due to the set
$\set{S}$ being intersection of halfplanes, does not have a straightforward solution as in the equality constraint case~\eqref{eq:example_projectedSubgradient_lambda_reprise}. The solution is found by applying the definition of the euclidean projection, which is by itself a \qp{} problem

\begin{equation}
  \Proj^{\set{X}}(\vec{x}_{0})=
  \argmin\limits_{\vec{x}}
  \left\{
  \begin{matrix}
    \minimize\limits_{\vec{x}} &
                                    \norm{\vec{x}-\vec{x}_{0}}^{2}\\
    \mathrm{subject~ to} & \vec{x}\in\set{X}\\
  \end{matrix}
  \right\},
\end{equation}
which translates to the problem
\begin{equation}
  \label{eq:projected_subgradient_qp}
  \Proj^{\set{S}}(\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p)=
  \argmin\limits_{\vec{x}}
  \left\{
  \begin{matrix}
    \minimize\limits_{\vec{U}[k]} &
                                    \norm{\vec{x}-\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p}\\
    \mathrm{subject~ to} & I_{c}^{M}\vec{x}\preceq \vec{U}_{\max}:\vec{\mu}\\
  \end{matrix}
  \right\},
\end{equation}
with corresponding dual variable $\vec{\mu}$.
For notation sake we use $\vec{x}_{0}=\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p$.

The problem is solved similarly to how we made in \S\ref{sec:impact-local-problem}.
Skipping some steps (calculating lagrangians and optimizing for the dual variables) we will find that the solution of $\vec{\mu}$ has the same form that~\eqref{eq:lambdafuntheta}.
However, we need to change $\bar{\Gamma}_{i}$ for $I_{c}^{M}$, $H_{i}$ for $I$, and $\vec{f}_{i}$ for $-(\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p)$.
And the partitions are defined by the active and inactive constraints in $I_{c}^{M}\vec{x}_{0}\preceq \vec{U}_{\max}$.

Substituting the solution of the dual variables into the lagrangian, and making the gradient vanish, will also result in a \pwa{} function with at most $2^{c}$ different modes (${\card{\elem[*,1]{I_{c}^{M}}}=c}$):
\begin{equation}
  \label{eq:projection_result_zones}
  \begin{aligned}
    \vec{\theta}[k]\pplusone=
    \begin{cases}
      \vec{x}_{0}+
      {I_{c}^{M}}^{(0)}\left[-P_{\vec{\mu}}^{(0)}\vec{U}_{max}+\vec{s}_{\vec{\mu}}^{(0)}[k]\right],&\text{if}\ G_{\vec{\mu}}^{0}[k]\vec{x}_{0} \preceq \vec{U}_{\max}^{0}[k] \\
      \qquad\quad \vdots&\qquad\quad \vdots\\
      \vec{x}_{0},&\text{if}\ G_{\vec{\mu}}^{2^{c}-1}[k]\vec{x}_{0} \preceq \vec{U}_{\max}^{2^{c}-1}[k] \\
    \end{cases}
  \end{aligned},
\end{equation}
where the non-zero elements (coded by $0$ digits in the binary representation of $(n)$) of $P_{\vec{\mu}}^{(n)}$ are ${({I_{c}^{M}}^{(n)}{{I_{c}^{M}}^{(n)}}\T)}^{-1}$, and those of $\vec{s}_{\vec{\mu}}^{(n)}[k]$ are $P_{\vec{\mu}}^{(n)}{I_{c}^{M}}^{(n)}\vec{x}_{0}$, ${I_{c}^{M}}^{(n)}$ is constructed by keeping/removing the constraints as coded by index $(n)$, and the halfspaces described by tuples $(G_{\vec{\mu}}^{n},U_{\max}^{n})$ represent the different regions of the partitions of the solution space of $\vec{x}_{0}$.

Observe that when all constraints are inactive ($n=0$), the solution is the same as in the equality case~\eqref{eq:euclidean_projection_equality_solution} ($\card{I_{c}^{M}}=c\times Mc$).
And when all constraints are active ($n=2^{c}-1$), the solution is the same as the unconstrained solution, since $\vec{\mu}=\0$ (see Remark~\ref{rem:sparse_solutions}).

So, we can conclude that the next value $\thetaik\pplusone$ depends on where $\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p$ will lie, giving at least $2^{c}$ possibilities.
In turn, $\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p$ depends on the values of each $\lambdaik$, which conversely can have $2^{\nineq}$ different modes each.

As in \S\ref{sec:analysis-negotiation}, when substituting the equations one in another gave us expressions of \dt{} systems, similarly here if we substitute the results for $\lambdaik$ in~\eqref{eq:projection_result_zones} we will have a switched \dt{} systems.
One with constant (in the same mode) input for $\thetaik$, and conversely a switched homogenous system for the dynamics of $\lambdaik$.

If we develop the calculations we can estimate an upper bound on the number of total different permutations $\bar{\epsilon}$, which totals to
\begin{equation}
  \label{eq:1}
  \bar{\epsilon}=\overbrace{2^{c}}^{\text{modes on projection}}\times \underbrace{2^{\nineq}\times \dots \times 2^{{\nineq}}}_{M\times \text{modes in each }\lambdai }=2^{c+M\nineq}.
\end{equation}

\begin{remark}
  Probably some of those permutations may not be possible, but still we have exponential growth on the number of modes of the switched systems.
  The exact calculation for the regions is not the scope of this work.
\end{remark}

The case for scarce systems seen in the last chapter, settles the solution of the projection to the case $(0)$, where $\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p$ is projected onto the intersection of all halfspaces.
And due to another assumption (that $\bar{\Gamma}_{i}\optuncUik\succ \thetaik, \forall i\in\set{M}, \forall k$), we fix all $\lambdai$ to the first solution in~\eqref{eq:lambdafuntheta}.
This consecutive choices of assumptions reduces the total number of switching modes to exactly one, presented by equations~\eqref{eq:negotiation_equation_substituting_organizing_matrix_lambda} and~\eqref{eq:negotiation_equation_substituting_organizing_matrix_theta}, reproduced here.

\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing_matrix_lambda_reprise}
  \tag{\ref*{eq:negotiation_equation_substituting_organizing_matrix_lambda}}
  \vec{\lambda}\pplusone=\mathcal{A}_{\lambda}\vec{\lambda}\p,
\end{equation}
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing_matrix_theta_reprise}
  \tag{\ref*{eq:negotiation_equation_substituting_organizing_matrix_theta}}
  \vec{\theta}\pplusone=\mathcal{A}_{\theta}\vec{\theta}\p+\mathcal{\vec{B}}_{\theta}[k]
\end{equation}

If we concatenate the binary representations of the modes of the projection and of each local problem, we will have a number with $c+M\nineq$ digits.
And thus, the solutions in ~\eqref{eq:negotiation_equation_substituting_organizing_matrix_lambda} and~\eqref{eq:negotiation_equation_substituting_organizing_matrix_theta} can be seen as an specific case of the following solutions for the $0$-th modes:
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing_matrix_lambda_switched}
  \vec{\lambda}[k]\pplusone=
  \begin{cases}
    \mathcal{A}_{\lambda}^{0}\vec{\lambda}[k]\p,&\text{if}\ G_{\vec{\lambda}}^{0}[k]\vec{\theta}[k]\p \preceq \vec{b}_{\vec{\lambda}}^{0}[k] \\
    \vdots\\
    \mathcal{A}_{\lambda}^{N_{\vec{\lambda}}}\vec{\lambda}[k]\p,&\text{if}\ G_{\vec{\lambda}}^{N_{\vec{\lambda}}}[k]\vec{\theta}[k]\p \preceq \vec{b}_{\vec{\lambda}}^{N_{\vec{\lambda}}}[k] \\
  \end{cases}
\end{equation}
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing_matrix_theta_switched}
  \vec{\theta}[k]\pplusone=
  \begin{cases}
    \mathcal{A}_{\theta}^{0}\vec{\theta}[k]\p+\mathcal{\vec{B}}_{\theta}^{0}[k],&\text{if}\ G_{\vec{\theta}}^{0}[k]\vec{\theta}[k]\p \preceq \vec{b}_{\vec{\theta}}^{0}[k] \\
    \vdots\\
    \mathcal{A}_{\theta}^{N_{\vec{\theta}}}\vec{\theta}[k]\p+\mathcal{\vec{B}}_{\theta}^{N_{\vec{\theta}}}[k],&\text{if}\ G_{\vec{\theta}}^{N_{\vec{\theta}}}[k]\vec{\theta}[k]\p \preceq \vec{b}_{\vec{\theta}}^{N_{\vec{\theta}}}[k] \\
  \end{cases}
\end{equation}
where $N_{\vec{\lambda}}=N_{\vec{\theta}}=2^{c+M\nineq}-1$, the hyperspaces described by tuples
$(G_{\vec{\lambda}}^{n},\vec{b}_{\vec{\lambda}}^{n})$ and
$(G_{\vec{\theta}}^{n},\vec{b}_{\vec{\theta}}^{n})$ coincide, and are formed by the superposition of the halfspaces in~\eqref{eq:projection_result_zones} and~\eqref{eq:lambdafuntheta}.
We highlight again that not necessarily all permutations are possible.

Observe that in some cases, it is straightforward to define the values of the matrices in~\eqref{eq:negotiation_equation_substituting_organizing_matrix_lambda_switched} and~\eqref{eq:negotiation_equation_substituting_organizing_matrix_theta_switched}.
The $0$-th case, as mentioned, is the same we found in the last chapter, giving
$\mathcal{A}_{\vec{\lambda}}^{0}=\mathcal{A}_{\vec{\lambda}}$,
$\mathcal{A}_{\vec{\theta}}^{0}=\mathcal{A}_{\vec{\theta}}$, and
$\vec{B}_{\vec{\theta}}^{0}[k]=\vec{B}_{\vec{\theta}}[k]$.
Similarly for the $N_{\vec{\theta}}$-th case where all the constraints of all systems are active (implying all $\lambdai=\0$), and the projection of $\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p$ is itself, yielding
$\mathcal{A}_{\vec{\lambda}}^{N_{\vec{\lambda}}}=0_{\card{\vec{\lambdai}}}$,
$\mathcal{A}_{\vec{\theta}}^{N_{\vec{\theta}}}=I_{\card{\vec{\thetai}}}$, and
$\vec{B}_{\vec{\theta}}^{N_{\vec{\theta}}}[k]=\0$.
The $N_{\vec{\theta}}$-th case is trivial since it means the optimal value was already found, $\vec{\theta}[k]\pplusone=\vec{\theta}[k]\p$.

Applying the attack ~\eqref{eq:linear_attack}, using cheating matrices $T_{i}$ we can have simular results

\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing_matrix_lambda_switched_cheating}
  \tilde{\vec{\lambda}}[k]\pplusone=
  \begin{cases}
    \tilde{\mathcal{A}}_{\lambda}^{0}\tilde{\vec{\lambda}}[k]\p,&\text{if}\ G_{\vec{\lambda}}^{0}[k]\vec{\theta}[k] \preceq \vec{b}_{\vec{\lambda}}^{0}[k] \\
    \vdots\\
    \tilde{\mathcal{A}}_{\lambda}^{N_{\vec{\lambda}}}\tilde{\vec{\lambda}}[k]\p,&\text{if}\ G_{\vec{\lambda}}^{N_{\vec{\lambda}}}[k]\vec{\theta}[k] \preceq \vec{b}_{\vec{\lambda}}^{N_{\vec{\lambda}}}[k] \\
  \end{cases}
\end{equation}
\begin{equation}
  \label{eq:negotiation_equation_substituting_organizing_matrix_theta_switched_cheating}
  \vec{\theta}[k]\pplusone=
  \begin{cases}
    \tilde{\mathcal{A}}_{\theta}^{0}\vec{\theta}[k]\p+\mathcal{\tilde{\vec{B}}}_{\theta}^{0}[k],&\text{if}\ G_{\vec{\theta}}^{0}[k]\vec{\theta}[k] \preceq \vec{b}_{\vec{\theta}}^{0}[k] \\
    \vdots\\
    \tilde{\mathcal{A}}_{\theta}^{N_{\vec{\theta}}}\vec{\theta}[k]\p+\mathcal{\tilde{\vec{B}}}_{\theta}^{N_{\vec{\theta}}}[k],&\text{if}\ G_{\vec{\theta}}^{N_{\vec{\theta}}}[k]\vec{\theta}[k] \preceq \vec{b}_{\vec{\theta}}^{N_{\vec{\theta}}}[k] \\
  \end{cases}
\end{equation}
One can observe that since we multiply $\lambdai$ by $T_{i}[k]$, we do not change the partition of the spaces.
We can similarly see that
$\tilde{\mathcal{A}}_{\vec{\lambda}}^{0}=\tilde{\mathcal{A}}_{\vec{\lambda}}$,
$\tilde{\mathcal{A}}_{\vec{\theta}}^{0}=\tilde{\mathcal{A}}_{\vec{\theta}}$,
$\tilde{\vec{B}}_{\vec{\theta}}^{0}[k]=\tilde{\vec{B}}_{\vec{\theta}}[k]$, and all analysis made in the last chapter also holds for this mode.

\begin{remark}\label{rem:cheat_satisfied_problem}
  One interesting fact we can observe is, for cases where the corresponding $\lambdai$ is already $\0$, the multiplication of $T_{i}$ will not affect its value.
  A satisfied agent, i.e., an agent whose local problem has same solution than its unconstrained version, has no motive to get more resources, since the allocated resources will satisfy its needs.
  So, in the most extreme case, specifically the $N_{\vec{\theta}}$-th mode, we see that
  even if all agents cheat at the same time, there is no impact of the cheat, since they are all satisfied, $\lambdai=\0\ \forall i \in \set{M}$, resulting in
  $\tilde{\mathcal{A}}_{\vec{\lambda}}^{N_{\vec{\lambda}}}=\mathcal{A}_{\vec{\lambda}}^{N_{\vec{\lambda}}}$,
  $\tilde{\mathcal{A}}_{\vec{\theta}}^{N_{\vec{\theta}}}=\mathcal{A}_{\vec{\theta}}^{N_{\vec{\theta}}}$, and
  $\tilde{\vec{B}}_{\vec{\theta}}^{N_{\vec{\theta}}}[k]=\vec{B}_{\vec{\theta}}^{N_{\vec{\theta}}}[k]$.
\end{remark}

We could analyze stability in a case-by-case fashion, seeing what constraints are active/inactive for which agent and how the cheating matrices influence the stability.
But due to the extension of the analysis (exponential number of cases) we skip it for brevity's sake, and will use a single example, different from the trivial solution, to illustrate how it could be done.

Denoting as the zone (or region) $n$, or the $n$-zone, the polytope defined by the intersection of halfspaces ${\set{R}^{n}=\setbuild{x}{G_{\vec{\theta}}^{n}[k]\vec{\theta}[k] \preceq \vec{b}_{\vec{\theta}}^{n}[k]}}$, we retake our $2$-constraints example in Fig.~\ref{fig:constraints_partition_theta}.
Since there are $2$ constraints, we set for example $c=1$ and $\predhorz=2$, then we have at most $4$ different modes on the solution for $\lambdaicheat$:
\begin{equation}
  \lambdaicheat[k]=
  \begin{cases}
    -\Plinineqtilde      \thetaik-\sikineqtilde      ,&\text{if }\thetaik\in\set{R}^{0}\\
    -\Plinineqtilde[i][1]\thetaik-\sikineqtilde[i][1],&\text{if }\thetaik\in\set{R}^{1}\\
    -\Plinineqtilde[i][2]\thetaik-\sikineqtilde[i][2],&\text{if }\thetaik\in\set{R}^{2}\\
    -\Plinineqtilde[i][3]\thetaik-\sikineqtilde[i][3],&\text{if }\thetaik\in\set{R}^{3}\\
  \end{cases}\label{eq:1}
\end{equation}
where
\begin{align}
  \Plinineqtilde&=T_{i}{{(\linearcoefiineqnonzero)}^{-1}}&&=&&T_{i}{(\linearcoefi)}^{-1}\\
  \Plinineqtilde[i][1]&=T_{i} \left[
                               \begin{matrix}
                                 {{(\linearcoefiineqnonzero[1,*])}^{-1}}&0\\
                                 0&0
                               \end{matrix}
                                    \right]&&=&& \left[
                                             \begin{matrix}
                                               \elem[1,1]{T_{i}}{{(\linearcoefiineqnonzero[1,*])}^{-1}}&0\\
                                               \elem[2,1]{T_{i}}{{(\linearcoefiineqnonzero[1,*])}^{-1}}&0\\
                                             \end{matrix}
                                                  \right]\\
  \Plinineqtilde[i][2]&=T_{i} \left[
                               \begin{matrix}
                                 0&0\\
                                 0&{{(\linearcoefiineqnonzero[2,*])}^{-1}}\\
                               \end{matrix}
  \right]&&=&& \left[
           \begin{matrix}
             0&\elem[1,2]{T_{i}}{{(\linearcoefiineqnonzero[2,*])}^{-1}}\\
             0&\elem[2,2]{T_{i}}{{(\linearcoefiineqnonzero[2,*])}^{-1}}\\
           \end{matrix}
  \right]
  \\
  \Plinineqtilde[i][3]&=T_{i}
                               \left[
                               \begin{matrix}
                                 0&0\\
                                 0&0
                               \end{matrix}
                                    \right]&&=&&\left[
                                             \begin{matrix}
                                               0&0\\
                                               0&0
                                             \end{matrix}
                                                  \right]
\end{align}
\begin{align}
  \sikineqtilde[i][0]&=&&T_{i}\sikineq[i][0]&&=&&T_{i}{(\linearcoefi)}^{-1}\bar{\Gamma}_{i}H_{i}^{-1}\vec{f}_{i}[k]\\
  \sikineqtilde[i][1]&=&&T_{i} \sikineq[i][1]&&=&& \left[
                                                  \begin{matrix}
                                                    \elem[1,1]{T_{i}}{{(\linearcoefiineqnonzero[1,*])}^{-1}}\elem[1,*]{\bar{\Gamma}_{i}}H_{i}^{-1}\vec{f}_{i}[k]\\
                                                    \elem[2,1]{T_{i}}{{(\linearcoefiineqnonzero[1,*])}^{-1}}\elem[1,*]{\bar{\Gamma}_{i}}H_{i}^{-1}\vec{f}_{i}[k]\\
                                                  \end{matrix}
                                                       \right]\\
  \sikineqtilde[i][2]&=&&T_{i} \sikineq[i][2]&&=&& \left[
               \begin{matrix}
                 \elem[1,2]{T_{i}}{{(\linearcoefiineqnonzero[2,*])}^{-1}}\elem[2,*]{\bar{\Gamma}_{i}}H_{i}^{-1}\vec{f}_{i}[k]\\
                 \elem[2,2]{T_{i}}{{(\linearcoefiineqnonzero[2,*])}^{-1}}\elem[2,*]{\bar{\Gamma}_{i}}H_{i}^{-1}\vec{f}_{i}[k]\\
               \end{matrix}
  \right]
  \\
  \sikineqtilde[i][3]&=&&T_{i}\sikineq[i][3] &&=&&\left[
                                                  \begin{matrix}
                                                    0\\ 0
                                                  \end{matrix}
                                                       \right]
\end{align}
with
\begin{align}
  \sikineq[i][0]&={{(\linearcoefiineqnonzero)}^{-1}}\elem[*,*]{\bar{\Gamma}_{i}}H_{i}^{-1}\vec{f}_{i}[k]\\
  \sikineq[i][1]&= \left[
                                \begin{matrix}
                                  {{(\linearcoefiineqnonzero[1,*])}^{-1}}\elem[1,*]{\bar{\Gamma}_{i}}H_{i}^{-1}\vec{f}_{i}[k]\\
                                  0
                                \end{matrix}
  \right]\\
  \sikineq[i][2]&=\left[
                                \begin{matrix}
                                  0\\
                                  {{(\linearcoefiineqnonzero[2,*])}^{-1}}\elem[2,*]{\bar{\Gamma}_{i}}H_{i}^{-1}\vec{f}_{i}[k]\\
                                \end{matrix}
  \right]\\
  \sikineq[i][3]&= \left[
                                \begin{matrix}
                                  0\\
                                  0
                                \end{matrix}
                                     \right]
\end{align}

One can observe that for zones $1$ and $2$, when the agent attacks, the non-zero elements of $\Plinineq[i][n]$ and $\sikineq[i][n]$, influences the other elements using the columns of $T_{i}$ equivalent to the inactive constraints.
For example in zone $1$, $\elem[1,1]{T_{i}}$ and $\elem[2,1]{T_{i}}$ are used, and for zone $2$, $\elem[1,2]{T_{i}}$ and $\elem[2,2]{T_{i}}$.
\begin{remark}
  Observe that if $T_{i}$ is diagonal, there is no injection of ``inactive elements'' into an ``active elements''.
\end{remark}

So, instead of always propagating to the negotiation all the matrix $T_{i}$, as in the case of the last chapter, only the columns related to the inactive constraints are propagated.

If we have $M=2$ subsystems, one in the $1$-zone and other in the $2$-zone (let us called them agents I and II) and we suppose during the projection ${G_{\vec{\mu}}^{0}[k](\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p) \preceq \vec{U}_{\max}^{0}[k]}$, we code the permutation as
\begin{equation}
\underbrace{00}_{\text{projection's zone}}\overbrace{01}^{\text{agent I`s zone}}\underbrace{10}_{\text{agent II's zone}} = 000110_{(2)}=6_{(10)}.
\end{equation}

Applying the projection, we will have the same solution~\eqref{eq:euclidean_projection_equality_solution}, reprised here
\begin{equation}
  \label{eq:euclidean_projection_equality_solution_reprise}
  \tag{\ref*{eq:euclidean_projection_equality_solution}}
    \vec{\theta}\pplusone=\vec{\theta}\p+\rho\p\vec{\lambda}\p+{I_{c}^M}\T/({I_{c}^M} {I_{c}^M}\T)\left({I_{c}^M}(\vec{\theta}\p-\rho\p\vec{\lambda}\p) -\vec{U}_{\max}\right),
\end{equation}
which can be rewritten for each $\thetai$ as
\begin{equation}
  \thetai\pplusone= \thetai\p + \frac{1}{M}\left(\sum_{j\in\set{M}}\vec{\theta}_{j}\p-\vec{U}_{\max}\right) + \rho\p\left(\vec{\lambda}\p-\frac{1}{M}\sum_{j\in\set{M}}\vec{\lambda}_j\p\right)
\end{equation}
and substituting the $\lambdai$ by $\lambdaicheat$:
\begin{align}
  \vec{\theta}_{i}\pplusone=
  \vec{\theta}_{i}\p+
  \frac{1}{M}\left(\sum_{j\in\set{M}}\vec{\theta}_{j}\p-\vec{U}_{\max}\right)&
                                                                     + \rho\p\left(
                                                                     -\Plinineqtilde[i][n(i)]\thetai\p-\sikineqtilde[i][n(i)]
                                                                     \right) \notag\\
                                                                   &-\frac{\rho\p}{M}\sum_{j\in\set{M}}\left(
                                                                     -\Plinineqtilde[j][n(j)]\vec{\theta}_{j}\p-\sikineqtilde[j][n(j)]
                                                                     \right)
\end{align}
where $n(i)$ indicates the coding of the zone of agent $i$.

Grouping the terms we have
\begin{align}
  \thetai\pplusone=\thetai\p+\rho\p\left(-\Plinineqtilde[i][n(i)]\thetai\p+\frac{1}{M}\sum_{j\in\set{M}}\Plinineqtilde[j][n(j)]\vec{\theta}_{j}\p\right)+\frac{1}{M}\left(\sum_{j\in\set{M}}\vec{\theta}_{j}\p\right)\\+\rho\p\left(-\sik+\frac{1}{M}\sum_{j\in\set{M}}\vec{s}_{j}[k]\right) -\frac{\vec{U}_{\max}}{M}
\end{align}
resulting in
\begin{equation}
  \vec{\theta}\pplusone=\tilde{\mathcal{A}}_{\theta}^{(6)}\vec{\theta}\p+\tilde{\mathcal{\vec{B}}}_{\theta}^{(6)}[k]
\end{equation}
where
\begin{equation}
\tilde{\mathcal{A}}_{\theta}^{(6)}=\left[
\begin{matrix}
2I-\frac{1}{2}\rho\p \Plinineqtilde[I][1] & I+\frac{1}{2}\rho\p \Plinineqtilde[II][2]\\
I+\frac{1}{2}\rho\p \Plinineqtilde[I][1] & 2I-\frac{1}{2}\rho\p \Plinineqtilde[II][2]\
\end{matrix}
\right]
\end{equation}
\begin{equation}
\tilde{\mathcal{\vec{B}}}_{\theta}^{(6)}[k]=\left[
\begin{matrix}
-\frac{1}{2}\rho\p \sikineqtilde[I][1]+\frac{1}{2}\rho\p \sikineqtilde[II][2]-\frac{\vec{U}_{\max}}{2}\\
\frac{1}{2}\rho\p \sikineqtilde[I][1]-\frac{1}{2}\rho\p \sikineqtilde[II][2]-\frac{\vec{U}_{\max}}{2}
\end{matrix}
\right]
\end{equation}

And we can see that, as expected, the columns of $T_{i}$ are propagated to $\mathcal{A}_{\vec{\theta}}$, and may change its dynamics.
We could do the same for the dynamics of $\lambdai$, but we let this for the reader.

For the overall system dynamics we may use any tools for analysis of hybrid/ switched systems~\cite{BorrelliEtAl2017}.

\begin{remark}
  Observe that since the matrices $\mathcal{A}_{\vec{\theta}}^{n}$ and $\mathcal{A}_{\vec{\lambda}}^{n}$ may be sparse, the eigenvalue analysis to indicate stability of the mode must be accomplished by removing the non-zero corresponding blocks.
\end{remark}

  % =\Proj^{\set{S}}(\vec{\theta}[k]\p+\rho\p\vec{\lambda}[k]\p),
As we see, the relaxation of the assumption makes the problem exponentially more complicated.
In the next section we will discuss ways to adapt the detection and mitigation strategies to overcome the complexity presented.

\section{Adapting detection and mitigation methods}\label{sec:detection-mitigation}

\subsection{Mitigation}\label{sec:mitigation_ineq}

\subsection{Detection}\label{sec:detection_ineq}

\newcommand{\thetairestricted}{\accentset{\rotatebox{90}{r}\note{choose a notation, please}}{\thetai}}
We assume that there is a value $\thetairestricted$ for each the local problem
with $\thetai=\thetairestricted$ is active.
$\optuncUik\in\set{U}_{i}$
We call this point the \emph{artificial scarcity point}.

if $\thetai=0$, constraint will be active.
\emph{artificial scarcity}

\begin{equation}
  \label{eq:lambda_reconstruction}
  {\vec{\lambda}_{i}}_{\mathrm{rec}}=\widehat{{T_{i}[k]}^{-1}} \tilde{\vec{\lambda}_{i}}
\end{equation}

\section{Considerations about Multiple Parameter Estimation}\label{sec:cons-about-mult}


We can use a method similar to the one seen in~\cite{NakadaEtAl2005}, where we can identify parameters of a switching system.

the submatrix generated from removing corresponding constraints

By definition we have


\section{Conclusion}\label{sec:conclusion}



\end{document}
