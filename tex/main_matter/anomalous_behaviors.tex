\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter[Anomalous Behaviors --- What they are and how to combat]{Anomalous Behaviors:\\What they are?\\How to combat?}\label{sec:anomalous}
\epigraph{``What the hell is going on with our equipment?''
``It wasn't meant to do this in the first place.''}
{\textit{Half-Life}\\\textsc{Valve}}

The decompositions shown in the last chapters work in normal conditions, but we can analyze more interesting cases, such as when systems do not behave nominally.

In this chapter, we discuss briefly the causes of anomalous behaviors, how they can happen, and the primary means used in the literature to mitigate their effects.

\minitoc%

\section{What are anomalous behaviors?}
We define \emph{\anomalousbehaviors} (or non-compliant behaviors) as any non-expected (change of) behavior of a system.

There are two primary causes for a system not to perform as nominally expected: \emph{faults} and \emph{attacks}.
The main difference between these two is \emph{intention}; while faults happen unintentionally, uncoordinated and with no objective, attacks happen intentionally, usually coordinated having a malicious objective.

\begin{remark}
  A non-expected behavior can also be observed when there are modeling errors, i.e., the theoretical model used to describe the system does not correspond to the real system.
  In this case, the system did not change its behavior, but the expectation that was false.
  So, for the time being we will ignore modeling errors and assume they do not cause the non-nominal behaviors.
\end{remark}

Both faults and attacks can deviate the overall system from its nominal optimal behavior, but the most severe effect they can produce is a complete breakdown.
In \cps{}, as the systems are connected to physical components, a malfunction can provoke a great impact on the population, since some of the systems can control important
aspects of modern life, as water and energy supply.

That's why we are interested in assuring the normal operation of such systems.

\section{Security}\label{sec:security}
Security alludes to the safe and reliable operation of a system even under unexpected circumstances, such as faults and attacks.
To understand security of \cps{}, we can first borrow some definitions from cyber-security and then extend them to \cps{}.

Computer systems are usually described in terms of data and resources, and \emph{cyber-security} is ensured by three pillars: \CIA{} of such components~\cite{Bishop2005}.

\paragraph{Confidentiality} refers to how undisclosed the data and resources are.
Sometimes parts of the system need to be secret and only accessible for a selected group.
Be it for privacy reasons, to avoid disclose personal info, for instance, or even to prevent the exposition of possible vulnerabilities of the system (a pop culture example can be the Death Star Plans on the Star Wars Saga).

\paragraph{Integrity} means how trustworthy the data and resources are.
For example, a new equipment is more trustworthy than an old one. For data traffic, the integrity of the communication may be divided into the integrity of the information sent/received (data integrity) and integrity of identity of the source/recipient (called authentication).

\paragraph{Availability} relates to how accessible to use the data and resources are when desired.
For example, if one control is given to a system, it should be applied to it, and equally, if we need data from a sensor, it must be able to send it.
Furthermore, it is always good to call attention to, as exposed in~\cite{Bishop2005}, that ``an unavailable system is at least as bad as no system at all''.
\\~\\
The vulnerabilities of a system can compromise any of the three pillars, sometimes multiple at the same time. So, let us explore the sources of the vulnerabilities in \cps{}.

\section{Vulnerabilities in \cps{}}

As it is known, \cps{} have its components in both Cyber and Physical domains.
So, as in computer systems, we can divide them into data (the immaterial part) and resources (the material part). We can also divide them into more specific parts such as: \textbf{Transducers}, which are sensors and actuators (as motors, valves, cameras, antennae, thermocouples etc.);
\textbf{Channels}, used for communication (wires, air, tubes etc); \textbf{Connectors}, which connect components by transmitting some physical quantity (wires, circuit traces, cables, water pipes, etc.);
the \textbf{Software}, which is the logic used to operate the physical components (code, \plc{} logic etc.); and the \textbf{Controller}, which is the hardware running the software (\plc{}s, micro-controllers, computers etc.).

~\\Each one of the aforementioned components represents a source of vulnerability.
\\\textbf{Transducers} can be targets of sabotage, for instance an attacker can use a cold object to disrupt thermostat readings to increase the temperature of a room. But the components may also deteriorate with time, which can eventually cause a fault.
\textbf{Channels} and \textbf{connectors} can as well be targets of sabotage, as someone can interfere on the transmission to observe the traffic to gather information or intentionally interrupt it. But interruptions and corruptions might be caused by natural accidents such solar flares (causing radio blackouts) or as trees knocking down electricity cables or sharks eating underwater cables.
\textbf{Software} can also be a source of vulnerability due to \emph{bugs}, which can interrupt the normal functioning or even let a \emph{hacker} gain total control of the system.
The \textbf{controller}, if not well dimensioned (computing capacity), can also be a vulnerability.
If it receives more requisitions than expected, the system will be overloaded and will cease to respond.

It is worth emphasizing that in a \iot{} context, almost all sensors and actuators have embedded controllers and software, which can also be sources of vulnerabilities themselves.

Attackers can discover some of those vulnerabilities and use them in their favor. Due to this adversary behavior, in this work we will concentrate on attacks, mentioning faults eventually.

\section{Attacks in \cps{}}\label{sec:attacks}

From the definition of cyber-attacks~\cite{Bishop2005} and other
definitions used in this chapter, we can define an attack as an ill-intentioned action which uses vulnerabilities of a cyber-physical system to violate its security.
The perpetrator of the action is called attacker.

In a computer context~\cite{Bishop2005}, \Citeauthor{Bishop2005} divides cyber-attacks into ``four broad classes'':
disclosure, deception, disruption and usurpation.
\textbf{Disclosure} is the unauthorized access to information (break of confidentiality).
\textbf{Deception} tries to deceit making false data pass as true (break of data integrity).
\textbf{Disruption} interrupts or prevents the system to work correctly (break of availability). And \textbf{usurpation} is the unauthorized control of the system (break of authenticity).

In a \cps{}/control context~\cite{CardenasEtAl2008}, normally only the first 3 categories are used, calling them Deception, Disclosure, and~\DoS{} instead of disruption.
Sometimes, for instance~\cite{AminEtAl2009}, authors use only deception and \DoS{} to divided attacks, probably because disclosure attacks do not affect performance.
But as we are we going to see following, depending on the attacker disclosure attacks when combined with others can create some sneaky attacks.

This model of categorization is usually called the \DDD{} model, and authors in \cps{} also add physical attacks~\cite{DibajiEtAl2019}.
To demonstrate the \DDD{} model, the components of \cps{} are abstracted into 4 different parts: the physical system to be controlled, the controller, the communication channels, and the information trafficking by them  (as seen in Fig.~\ref{fig:cps_abstraction}).

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[
    node distance=.5cm and 2cm,
    box/.style={draw,align=center,minimum height=1.5cm,minimum width=3cm,rectangle,black},
		]

    \node[box] (plant) at (0,0) {Physical\\System};

    \node[box,below=of plant] (controller) {Controller};

    \draw[-latex,thick] (controller.west) -- ++(-1,0) node [midway,below] {${u}$} |- (plant.west) ;

    \draw[-latex,thick] (plant.east) -- ++(1,0) node [midway,above] {${y}$} |- (controller.east);
\end{tikzpicture}
\caption[General abstraction of CPS.]{General abstraction of \cps{}. (Adpated from~\cite{CardenasEtAl2008,AminEtAl2009})}\label{fig:cps_abstraction}
\end{figure}

In the diagram, $u$ corresponds to the control inputs calculated by the controller to be applied in the physical system by the actuators and $y$ corresponds to the outputs of the systems transmitted by the sensors.

Due to the networked nature of the systems, we modify the scheme to also model the communication between systems.
In Fig.~\ref{fig:networked_cps_abstraction} we represent all other systems as a single entity called network with which the system may communicate.
We also add two other signals, which represent the information shared with the network, $I_{\text{in}}$ is information received from all other systems and $I_{\text{out}}$ is the information send to all other systems.

\begin{figure}[h]
  \centering
\begin{tikzpicture}[node distance=1cm and 2cm,
    box/.style={draw,align=center,minimum height=1.5cm,minimum width=3cm,rectangle,black},
		]

    \node[box] (plant) at (0,0) {Physical\\System};

    \node[box,below=of plant] (controller)  {Controller};

    \node[draw,cloud,aspect=2,cloud puffs=20,below=1.5cm of controller] (network)  {Network};

    \draw[thick] (controller.west) -- ++(-1,0) node (a) {} node [midway,below] {${u}$};
    \draw[thick] (a.center) -- (a.center |- plant.west) node (b) {};
    \draw[-latex,thick] (b.center) -- (plant.west);

    \draw[thick] (plant.east) -- ++(1,0) node (aa) {} node [midway,above] {${y}$};
    \draw[thick] (aa.center) -- (aa.center |- controller.east) node (bb) {} ;
    \draw[-latex,thick] (bb.center) -- (controller.east);

    \draw[latex-,thick,dotted] (controller.south -| network.puff 20) -- +(0,-.25) node (nin) {} ;
    \draw[thick,dotted] (nin.center) -- ($(nin.center) + (0,-1)$) node (nin2) {};
    \draw[thick,dotted] (nin2.center) -- (network.puff 20) {};
    \node at ($(nin2)+(.45,.40)$) {$\mathcal{I}_{\text{in}}$};

    \draw[latex-,thick,dotted] (network.puff 2) -- +(0,.25) node (nout) {} ;
    \draw[thick,dotted] (nout.center) -- ($(nout.center) + (0,1)$) node (nout2) {};
    \draw[thick,dotted] (nout2.center) -- ( controller.south -| network.puff 2) {};
    \node at ($(nout)+(-.45,.5)$) {$\mathcal{I}_{\text{out}}$};
  \end{tikzpicture}
  \caption{General abstraction of networked \cps{}.}\label{fig:networked_cps_abstraction}
\end{figure}

Using this general networked control system scheme, the \DDD{} model can be characterized by where and how the attack happen.
In Fig.~\ref{fig:attacks_networked_cps} we can see multiple kinds of attacks enumerated from A1 to A13.
The attacks from A1 to A4 are disclosure attacks, which happen in the information level. The attacker observes the traffic, maybe to a posterior use.
This attack is also known as a \textbf{snooping} or \textbf{eavesdropping} attack.
A5 to A8 represent deception attacks.
The attacks also happen in the information level, but the attacker changes the signals to a modified version (here represented by a \~{} above the variable name).
The attacks from A10 to A12 are disruption (or \textbf{\DoS{}}) attacks, where the attacker attacks the communication channel, momentarily or not preventing the communication between two entities.
And A13 are physical attacks, where the physical system is attacked.
In this attack, the attacker change physically the behavior of the system.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[node distance=1cm and 2cm,
    box/.style={draw,align=center,minimum height=1.5cm,minimum width=3cm,rectangle,black},
		]

    \node[thick,box] (plant) at (0,0) {Physical\\System};

    \node[thick,box,below=of plant] (controller)  {Controller};

    \node[thick,draw,cloud,aspect=2,cloud puffs=20,below=2.0cm of controller] (network)  {Network};

    \draw[thick] (controller.west) -- ++(-1,0) node (a) {} node [midway,below] (input) {$\tilde{u}$};
    \draw[thick] (a.center) -- (a.center |- plant.west) node (b) {};
    \draw[-latex,thick] (b.center) -- (plant.west);
    \node (switch_input) at ($(a)!0.5!(b)$) {};

    \draw[thick] (plant.east) -- ++(1,0) node (aa) {} node [midway,above] (output) {$\tilde{y}$};
    \draw[thick] (aa.center) -- (aa.center |- controller.east) node (bb) {} ;
    \draw[-latex,thick] (bb.center) -- (controller.east);
    \node (switch_output) at ($(aa)!0.5!(bb)$) {};


    \draw[latex-,thick,dotted] (controller.south -| network.puff 20) -- +(0,-.5) node (nin) {} ;
    \draw[thick,dotted] (nin.center) -- ($(nin.center) + (0,-1)$) node (nin2) {};
    \draw[thick,dotted] (nin2.center) -- (network.puff 20) {};
    \node (info_in) at ($(nin)+(.45,-.10)$) {$\widetilde{\mathcal{I}_{\text{in}}}$};

    \draw[latex-,thick,dotted] (network.puff 2) -- +(0,.5) node (nout) {} ;
    \draw[thick,dotted] (nout.center) -- ($(nout.center) + (0,1)$) node (nout2) {};
    \draw[thick,dotted] (nout2.center) -- ( controller.south -| network.puff 2) {};
    \node (info_out) at ($(nout)+(-.45,-.3)$) {$\widetilde{\mathcal{I}_{\text{out}}}$};

    \node[red,above right=1cm of aa,inner sep=1pt,align=center] (a1) {\scalebox{1.5}{\faUserSecret}};
    \draw[-latex,thick] (a1) -- (output) node [midway,above,sloped] {{\faEye} A1} node [midway,below,sloped] {{\faPencil} A5};
    % \draw[-latex,thick] ($(a1)+(0,.1)$) -- ($(output)+(0,.1)$)  node [midway,above,sloped] {A1\faEye};
    % \draw[-latex,thick] ($(a1)+(0,-.1)$) -- ($(output)+(0,-.1)$)  node [midway,below,sloped] {A1\faPencil};


    \node[red,right=1.2cm of info_in,inner sep=1pt,align=center] (a2) {\scalebox{1.5}{\faUserSecret}};
    \draw[-latex,thick] (a2) -- (info_in) node [midway,below,sloped] {{\faEye} A2} node [midway,above,sloped] {{\faPencil} A6};

    \node[red,left=1.3cm of info_out,inner sep=1pt,align=center] (a3) {\scalebox{1.5}{\faUserSecret}};
    \draw[-latex,thick] (a3) -- (info_out) node [midway,above,sloped] {A3 {\faEye}} node [midway,below,sloped] {A7 \reflectbox{\faPencil}};


    \node[red,below left=1cm of a,inner sep=1pt,align=center] (a4) {\scalebox{1.5}{\faUserSecret}};
    \draw[-latex,thick] (a4) -- (input) node [midway,above,sloped] {A4 {\faEye}} node [midway,below,sloped] {A8 \reflectbox{\faPencil}};


    \node[red,right=.4cm of switch_output,inner sep=1pt,align=center] (a5) {\scalebox{1.5}{\faUserSecret}};
    \node[inner sep=1pt,align=center] at ($(switch_output) + (.3,0)$) {\rotatebox[origin=c]{180}{\faCut}};
    \node [above,sloped] at ($(a5) + (-.05,-.8)$) {A9};

    \node[red,right=.4cm of nin2,inner sep=1pt,align=center] (a6) {\scalebox{1.5}{\faUserSecret}};
    \node[inner sep=1pt,align=center] at ($(nin2) + (.3,0)$) {\rotatebox[origin=c]{180}{\faCut}};
    \node [above,sloped] at ($(a6) + (-.05,-.8)$) {A10};


    \node[red,left=.4cm of nout2,inner sep=1pt,align=center] (a7) {\scalebox{1.5}{\faUserSecret}};
    \node[inner sep=1pt,align=center] at ($(nout2) + (-.3,0)$) {\rotatebox[origin=c]{0}{\faCut}};
    \node [above,sloped] at ($(a7) + (-.05,-.8)$) {A11};

    \node[red,left=.4cm of switch_input,inner sep=1pt,align=center] (a8) {\scalebox{1.5}{\faUserSecret}};
    \node[inner sep=1pt,align=center] at ($(switch_input) + (-.3,0)$) {\rotatebox[origin=c]{0}{\faCut}};
    \node [above,sloped] at ($(a8) + (-.05,-.8)$) {A12};


    \node[above=.5cm of plant,inner sep=1pt,align=center] (a9) {\hspace{1.5pt}\scalebox{1.5}{\reflectbox{\scalebox{.8}{\faWrench}}{\color{red}\faUserSecret}}};
    \draw[-latex,thick] (a9) -- (plant) node [midway,left] {A13};

\end{tikzpicture}
\caption[Attacks in networked \cps{}.]{Attacks in networked \cps{}. Adpated from~\cite{AminEtAl2009,DibajiEtAl2019}.}\label{fig:attacks_networked_cps}
\end{figure}

As argued in~\cite{CardenasEtAl2008}, to accomplish attacks A13, attackers need physical access to the plant.
Or the attackers need to be close to the plant themselves, or they use a teleoperated machine that is near the plant.
The risks involved into trespassing and interacting with the machine, or planting a machine can be discouraging for the attacker, since they can injure themselves or may be identified.
The attacks A1-A12, on the other hand, may present less risk since the attacker need only to have access to the communication channel.
\\For instance, nowadays as communication may happen through internet, attackers could snoop some information from the comfort of their houses.
They can use an open-source ``packet sniffer'' such as wireshark\footnote{\url{https://www.wireshark.org}}, which can read different protocols including the Fieldbus family.
Many of the distributed control systems use protocols from this family, for example PROFIBUS and PROFINET (used for \scada{} architectures connecting \plcs{}, \HMIs{} and Supervisors Interfaces), CAN (used for automobiles) and others.

In~\cite{TeixeiraEtAl2015}, the authors stress that the attacks not necessarily happen purely.
For instance an attacker could record the information trafficking in the communication channel (disclosure attack) to replay it in a future time, deceiving controllers and supervisors (deception attack) generating some kind of input in the system which can be harmful for the physical system.
This attack is commonly known as \textbf{Replay attack}~\cite{ZhuMartinez2014}.
One of the classic examples of this attack in the pop culture is heist movies, such as Ocean's Eleven, where the protagonists record the video feed of surveillance cameras to deceive security guards.
These kind of attack exemplify how deception attacks can be subtler and more ingenious than \DoS{} attacks, as stressed in~\cite{MoSinopoli2009}.

In~\cite{TeixeiraEtAl2015} the authors also emphasize how the knowledge of control and physical aspects of \cps{} (sensors, actuators and plant dynamics) can be advantageous for an attacker.
So, instead of using purely the \DDD{} model, \citeauthor{TeixeiraEtAl2015}~\cite{TeixeiraEtAl2015} proposed an attack space with 3 orthogonal dimensions: Model Knowledge, Disruption resources and Disclosure resources as seen in Fig.~\ref{fig:3_dimensions_attack}.

\usetikzlibrary{3d}
\usetikzlibrary{perspective}
\begin{figure}[h]
  \centering
  \begin{tikzpicture}[3d view={105}{15},
    grid/.style={very thin,gray},
    axis/.style={-latex,thick},
    cube/.style={very thick,fill=red},
    cube hidden/.style={very thick,dashed,lightgray!70}]

    % draw the axes
    \draw[axis] (0,0,0) -- (2,0,0) node[below left]{Disruption Resources};
    \draw[axis] (0,0,0) -- (0,2,0) node (disclosure) {};
    \node[above right=5pt and -10pt] at (disclosure) {Disclosure Resources};
    \draw[axis] (0,0,0) -- (0,0,2) node (model_knowledge) {};
    \node[above] at (model_knowledge) {Model Knowledge};

  \end{tikzpicture}
  \caption{3 dimensional attack space.}\label{fig:3_dimensions_attack}
\end{figure}

To illustrate, we can populate the attack space with some attacks, some already cited and others which will be explained subsequently (Fig.~\ref{fig:3_dimensions_attack_with_attacks}).

\begin{figure}[h]
  \centering
  \begin{tikzpicture}[3d view={105}{15},
    grid/.style={very thin,gray},
    axis/.style={-latex,thick},
    cube/.style={very thick,fill=red},
    cube hidden/.style={very thick,dashed,lightgray!70}]

    % draw the axes
    \draw[axis] (0,0,0) -- (5,0,0) node[below left]{Disruption Resources};
    \draw[axis] (0,0,0) -- (0,5,0) node (disclosure) {};
    \node[above right=5pt and -10pt] at (disclosure) {Disclosure Resources};
    \draw[axis] (0,0,0) -- (0,0,5) node (model_knowledge) {};
    \node[above] at (model_knowledge) {Model Knowledge};

    \draw[cube hidden] (2,0,0) -- (2,2,0);
    \draw[cube hidden] (0,2,0) -- (2,2,0);


    \draw[cube hidden] (0,4,4) -- (0,0,4); %% top face
    \draw[cube hidden] (0,4,4) -- (4,4,4); %% top face
    \draw[cube hidden] (4,0,4) -- (4,4,4); %% top face
    \draw[cube hidden] (4,0,4) -- (0,0,4); %% top face

    \draw[cube hidden] (4,4,0) -- (4,4,4); %%
    \draw[cube hidden] (4,4,0) -- (4,0,0);
    \draw[cube hidden] (4,4,0) -- (0,4,0);
    \draw[cube hidden] (0,4,4) -- (0,4,0);
    \draw[cube hidden] (4,0,4) -- (4,0,0);


    \draw[cube hidden] (3,0,4) -- (3,0,0);

    \draw[cube hidden] (0,4,3) -- (0,0,3); %% top face
    \draw[cube hidden] (0,4,3) -- (4,4,3); %% top face
    \draw[cube hidden] (4,0,3) -- (4,4,3); %% top face
    \draw[cube hidden] (4,0,3) -- (0,0,3); %% top face

    \draw[cube hidden] (2,0,1.5) -- (0,0,1.5); %% top face
    \draw[cube hidden] (2,0,1.5) -- (2,0,0); %% top face

    \node[fill,circle,inner sep=1.5pt] (eavesdropping) at (0,.5,0) {};
    \node[fill,circle,inner sep=1.5pt] (zero_dynamics) at (3,0,4) {};
    \node[fill,circle,inner sep=1.5pt] (false_data_injection) at (3,0,3) {};

    \node[fill,circle,inner sep=1.5pt] (dos) at (3,0,0) {};
    \node[fill,circle,inner sep=1.5pt] (covert) at (4,4,4) {};
    \node[fill,circle,inner sep=1.5pt] (soft_modif) at (4,4,3) {};
    \node[fill,circle,inner sep=1.5pt] (topology) at (2,0,1.5) {};

    \node[left=10pt] at (dos) {DoS};
    \node[left=10pt] at (topology) {Topology};
    \node[above right=5pt and -10pt] at (eavesdropping) {Eavesdropping};
    \node[above right=5pt and -30pt] at (covert) {Covert};
    \node[fill,circle,inner sep=1.5pt,label=below:Replay] at (2,2,0) {};
    \node[align=center,right=.2cm] at (soft_modif) {Software\\ Modification};
    \node[above left=5pt and -10pt] at (zero_dynamics) {Zero Dynamics};

    \node[align=center,left=10pt] at (false_data_injection) {False Data\\ Injection};

  \end{tikzpicture}
  \caption[Attack space and some attacks.]{Attack space and some attacks. (Adpated from~\cite{TeixeiraEtAl2015})}\label{fig:3_dimensions_attack_with_attacks}
\end{figure}

In~\textbf{\DoS{} attacks} the attacker interrupts the communication between entities directly by attacking the communication channel~\cite{SunYang2019,ZhaoEtAl2020,YangEtAl2019} or by making more requisitions to the controller than it can expect, this is also called~\textbf{spamming} or \textbf{jamming}.
Such attack causing what we call \textbf{packet drops}~\cite{ChenEtAl2021}.
Depending on the scale of the system (computational power), attackers may coordinate multiple computational units (usually geographically distributed) to make the requests, this is called a \dDoS{}~\cite{WangLu2013,HussainEtAl2021}.
\begin{remark}
  Observe that packet drops may also occur in some unreliable networks, sometimes called \textbf{lossy networks}.
  The study of communication in such networks is important since wireless communication (profusely used nowadays) usually present packet loss and acknowledgment to reception of packets with protocols such \tcpip{} needs synchronization which for some use cases may increase drastically delays~\cite{BofEtAl2019}.

\end{remark}

In \textbf{Topology attacks}, the attackers manipulate the topology of the system or misleads the control to believe the system is under a different topology (with false-data). Such attacks are done by changing the state of devices like switches and breakers~\cite{KimTong2013,WuEtAl2016,ZhangEtAl2021b}.
\\In a \textbf{Software Modification attack}, the attacker modifies the software used in the controller, as done by the attackers in the Stuxnet attack~\cite{Langner2011}.
\\In \textbf{\fdi{} attacks}, the attacker sends a false data to the controller or the plant, usually with some insight about the functioning of the system~\cite{PasqualettiEtAl2013}.
In an example shown in~\cite{TeixeiraEtAl2015}, the attackers objective is to inject a constant bias so it can affect the steady state value of the variables in the system.
\\In \textbf{Covert attacks}, the attacker fabricates new control and output signals to be fed to the plant and the controller.
For this the attacker uses a double of the plant (model) and all signals which are available.
This way the convert controller can choose a different reference and since the dynamics are close enough to the plant, the attack may go unnoticed (stealth attack)~\cite{Smith2015,HoehnZhang2016,BarboniEtAl2020} (See Fig.~\ref{fig:covert_attack})

\begin{figure}[h]
  \centering
  \scalebox{0.8}{
  \begin{tikzpicture}[node distance=1cm and 2cm,
    box/.style={thick,draw,align=center,minimum height=1.5cm,minimum width=3cm,rectangle,black},
    network/.style={thick,draw,cloud,cloud puffs=20,aspect=2},
    connection/.style={draw,fill,circle,inner sep=0pt,minimum width=3pt},
    ]

    \node[box] (plant) at (0,0) {Physical\\System};
    \node[box,below=7.5cm of plant] (controller)  {Controller};
    \node[network,above=.75cm of controller] (network)  {Network};
    \node[box,above=1cm of network] (covert_controller)  {Covert\\Controller};
    \node[box,above=.5cm of covert_controller] (plant_model)  {Plant\\Model};

    \node[thick,draw,circle,inner sep=1pt] (y_sum) at ($(covert_controller.east) + (2,-.5)$)  {\small $+$};
    \node[thick,draw,circle,inner sep=1pt,left=1.75cm of plant_model] (u_sum)  {\small $+$};

    \draw[thick] (controller.west) -- ++(-2,0) node (a) {} node [midway,below] (input) {$u$};
    \draw[thick] (plant.east) -- ++(2,0) node (aa) {} node [midway,above] (output) {$y$};
    \draw[-latex,thick] (aa.center) -- (y_sum);

    \draw[thick] (plant_model.east) -- ++(1,0) node (model_aa) {} node [midway,above] {$\gamma$};
    \draw[-latex,thick] (model_aa.center |- y_sum) node [connection] {} -- (y_sum) node[midway,below right=.1cm] {-};
    \draw[-latex,thick] (model_aa.center) |- (covert_controller.east |- y_sum);

    \draw[thick] ($(covert_controller.west) +(0,.5)$) -- ++(-1,0) node (covert_a) {} node [midway,below] {$\mu$};
    \draw[-latex,thick] (covert_a.center) |- (plant_model.west);
    \draw[-latex,thick] (covert_a.center|-u_sum) node[connection] {} -- (u_sum);

    \draw[-latex,thick] (u_sum |- y_sum) node[connection] {} -- (covert_controller.west|- y_sum);


    \node[connection] (y_to_covert) at ($(y_sum) + (0,1)$) {};
    \draw[-latex,thick] (model_aa.center) |- (covert_controller.east |- y_sum);
    \draw[-latex,thick] (y_to_covert) |- (covert_controller.east |- y_to_covert);

    \draw[-latex,thick] (y_sum) |- (controller.east);


    \draw[-latex,thick] (a.center) -- (u_sum);
    \draw[-latex,thick] (u_sum) |- (plant.west);

    \draw[latex-,thick,dotted] (controller.north -| network.puff 12) -- (network.puff 12) node[midway,right] {$\mathcal{I}_{\text{in}}$} ;
    \draw[latex-,thick,dotted] (network.puff 10) -- ( controller.north -| network.puff 10) node[midway,left] {$\mathcal{I}_{\text{out}}$} ;
    \draw[latex-,thick,dotted] (covert_controller.south -| network.puff 20) -- (network.puff 20) node[midway,right] {$\mathcal{I}_{\text{in}}$} ;
    \draw[latex-,thick,dotted] (network.puff 2) -- ( covert_controller.south -| network.puff 2) node[midway,left] {$\mathcal{I}_{\text{out}}$} ;
\end{tikzpicture}
}
\caption[Covert agent.]{Covert agent. Adpated from~\cite{Smith2015,BarboniEtAl2020}.}\label{fig:covert_attack}
\end{figure}

As we will soon discuss, to counter these attacks sometimes we use detectors of anomalous behaviors which tries to detect a change in the dynamics of the system (plant plus controller)~\cite{TeixeiraEtAl2012,TeixeiraEtAl2015,HoehnZhang2016}, more generically an attack that can go unnoticed under these detectors is called a \textbf{Zero Dynamics Attack}.

The cited attacks are not extensive but gives a background of the more usual attacks presented in the literature.
Other attacks can be seen in~\cite{TeixeiraEtAl2015,ZhangEtAl2021b} and all other cited works in this chapter.

\section{Attacks in \dmpc{}}\label{sec:attacks_in_dmpc}
As pointed in~\cite{ArauzEtAl2021}, although cyber-physical security has been studied in \mpc{} frameworks~\cite{SunYang2019,FranzeEtAl2022}, the \dmpc{} community has not yet studied it sufficiently.
While many decompositions methods are discussed, the agents usually are considered cooperative and no vulnerabilities are explored.
But some effort has been made in the recent years to explore the vulnerabilities of some decompositions methods, noticeably dual decomposition~\cite{VelardeEtAl2017b,VelardeEtAl2017a,VelardeEtAl2018,AnandutaEtAl2018,AnandutaEtAl2019,AnandutaEtAl2020} (which is the most widely used decomposition) and Jacobi-Gau√ü decomposition~\cite{ChanfreutEtAl2018}.

In all these works the attacks are not originated from an external agent.
One (or a group) of the agents  behave non-cooperatively and tries to steer de decomposition based on a objective, be it to destabilize the system or to benefit a group of agents.
These kinds of attacks are called \textbf{inter-agent attacks} and sometimes considered as \textbf{insider attacks} (as if someone with sufficient access rights has taken control).
Usually the objectives of the attacker are accomplished by deception attacks.

The mentioned works present \fdi{} attacks, which they categorize based on where in the optimization problem the deception occurs.
\\\textbf{Selfish attack} is when the attacker modify its local objective function (usually multiplying it by a positive constant) so it can be more penalized, which causes all other agents to increase the efforts to balance this disadvantage and decrease the local cost of the attacker.
\\In \textbf{Fake weights attacks}, the non-cooperative agent uses different weights in its local objective function. A selfish attack can be viewed as a specific case of this kind of attack.
\\Supposing the agents follow a reference, in a \textbf{false reference attack} the malicious agent uses a different value of reference to calculate its local objective function.
\\For a \textbf{fake constraints attack}, the attacker uses different constraints when solving its local optimization problem.
\\The authors call a \textbf{``liar'' agent attack} when, after the negotiation is finished as usual, the attacker uses a control different from the agreed one.

In this work, we choose to also study \fdi{} attacks, but using other approach for security.
\section{Securing \cps{}}\label{sec:maintaining_security}
The first thing to remember is that nothing is safe and everything can be potentially attacked.
The initial step is to assess the risks and see which parts of your system you trust.
Some parts of the system can be more \emph{vulnerable} to \emph{threats}, i.e. some components have more probability of being attacked or to suffer from faults.
As a second measure, we usually protect this components that have more probability of presenting an anomalous behavior~\cite{Bishop2005} in order to prevent such behaviors.
In~\cite{WangLu2013} some risk assessment methods for smart-grids are presented.

\paragraph{Prevention} To prevent anomalous behaviors, we secure the vulnerable components.
Some examples follow.
Tampering of \emph{physical components} is prevented by \emph{enclosing} them by walls and doors with \emph{access control} and adding \emph{monitoring devices} (cameras and alarms)~\cite{CardenasEtAl2008,DingEtAl2018}, persuading the attacker to not approach such components.
Faults due to \emph{deterioration} are prevented usually by a \emph{periodical preventive maintenance}~\cite{ChenEtAl2021}, and substitution of deteriorated components by new ones.
Attacks and faults caused by \emph{software vulnerabilities} may be prevented by \emph{corrective patches} that are sent to all users of the software to correct the bugs.
Another method is \emph{software rejuvenation}~\cite{AlonsoEtAl2012,GriffioenEtAl2020}, where the system is refreshed with a trusted copy of the original software periodically, useful when there is a chance the software was corrupted.
For communication/transmission, we increase the robustness of the mean, by using better cables with insulation and braided shields for example.
To secure exchanges usually \emph{cryptography} is used to ensure data integrity and authenticity of agents~\cite{DingEtAl2018}. Other option to secure data is the
\emph{blockchain} (mentioned in~\ref{sec:drawbacks}), which the proper conception can protect against deception attacks or data corruption.
Other method to prevent deception attacks is to use a game-theoretic approach~\cite{MaestreEtAl2011,ZhuMartinez2011} which may also disincentivizes the attacker from being greedy since it knows that potentially all other agents could also be greedy. This approach can result in suboptimality.

~\\If prevention fails it is need to have another strategy to maintain security.
In the literature, those strategies are usually accompanied by one of two words: robustness and resilience.
Although both terms are generally used interchangeably in the literature, here we make a distinction.
\textbf{Robustness} can be defined as the ability to withstand perturbations without change in function~\cite{Jen2003}. \textbf{Resilience} is the ability to maintain an accepted level of operational normalcy in  response  to  disturbances,  including  threats  of  an unexpected and malicious nature~\cite{Rieger2010}.

Here we interpret robust methods as more passive strategies, while resilient methods are more adaptive, active in order to reestablish a more optimal behavior.
So we can divide the security methods into two categories: active and passive

\subsection{Passive strategies}\label{sec:protecting_against_attacks}
Passive methods are usually based on robust control, where the control law implemented does not change in presence of faults or attacks.
It can maintain satisfactory performance (with loss of optimality) if values of some project variables are inside a given safety range or if a certain maximum number of components present anomalous behaviors.

The majority of the robust strategies is used for \ftc{}, however, depending on the similarities between faults and the attacks, some robust control approaches can be used, as indicated in~\cite{TeixeiraEtAl2015,DingEtAl2018,ArauzEtAl2021}.

For example, a \fdi{} attack could change the value of a variable just as a fault or malfunction could add a disturbance on the signal, so some passive \ftc{} techniques such as the one shown in~\cite{VahidNaghaviEtAl2014} could also be used provided that the data injected respects the bounds set by the method,

Another example is in distribution grids where there is a N-1 static security index, which indicates how stable the grid is even if 1 component present a failure (or is disconnected)~\cite{QianEtAl2022}.
So, for a system said N-1 safe, if a component is attacked (a \DoS{} for example) the system still can present satisfactory performance.
\citeauthor{VelardeEtAl2018} in~\cite{VelardeEtAl2017b,VelardeEtAl2018} used a similar solution based on f-robustness of graphs\footnote{Graph maintain connectivity even if a number $f$ of agents is disconnected}~\cite{DibajiIshii2015,WangIshii2019} for robust \dmpc{} based on dual decomposition. During each step of the negotiation, extreme values (the f biggest and smallest values) are ignored.

Yet another strategy is the one in~\cite{VelardeEtAl2017a,MaestreEtAl2021}, where trustworthy historical scenarios (\mpc{} trajectories) with known occurrence probabilities are stored and used to solve the problem together with the current calculation, which may not be so reliable due to attacks.

\subsection{Active strategies}\label{sec:protecting_against_attacks}

For other specific kinds of anomalous behaviors, e.g.\ more ingenious attacks such as replay attacks or covert attacks, some resilient methods need to be created.
The active methods are usually bi-modal.
When there is no attack it behave in some way, and when an attack happens it changes its behavior.

In order to identify an anomalous behavior, first we need some prior information about the normal behavior of the system, for example the dynamics of certain variables, bounds or any other useful information.
With this knowledge we can supervise the threatened components and create monitors.
This part is called the \textbf{Detection} phase, where the attack can be detected through the monitors/detectors and sometimes there is an isolation step, in which the misbehaving component/agent is identified.
In some specific cases the isolation can occur during the detection phase.

Once an attack is detected, the control law changes in order to mitigate the effects of the anomalous behavior.
This is called the \textbf{Mitigation} or \textbf{Recovery} phase.

In the next subsections we give some examples of mechanisms to better illustrate both phases.

\subsection{Detection}

The detection phase techniques can be categorized in some ways.
For example depending if an additional signal to the monitored data is added (\textbf{active detection}) or not (\textbf{passive detection}).

The techniques can also be divided by they use event-triggered approaches or not.
We give the works in~\cite{SunYang2019,HuEtAl2021,SunEtAl2022} as examples of event-triggered methods, but this work do not focus on this kind of methods.

Other categorization is if the monitoring devices use analytical knowledge of the system or use learning methods.

For active detection we can mention the \emph{watermarking} where a (pseudo) random authentication signal is superimposed to the monitored signal we want to secure~\cite{MoSinopoli2009,MoEtAl2015,SatchidanandanKumar2017,KshetriVoas2017,LuciaEtAl2021}, it can protect against replay attacks since with the knowledge of the random seed it is possible to have a unique timestamp.

Some passive and analytical examples can be the observators used in~\cite{HoehnZhang2016} or residues of state dynamics~\cite{TeixeiraEtAl2015,BoemEtAl2020}, which can be associated with hypothesis testing~\cite{MoSinopoli2009} ($\chi^{2}$ detectors), or other
set membership detection as in~\cite{FortiEtAl2016} or~\cite{MaestreEtAl2018} for instance.
Some more examples are seen in~\cite{PasqualettiEtAl2012,PasqualettiEtAl2013,ZhangEtAl2021a,ArauzEtAl2021}.

For learning methods we can reference~\cite{AnandutaEtAl2018,AnandutaEtAl2019,AnandutaEtAl2020} which uses
a bayesian approach to learn bounds and detect suspect agents.
Other example is training neural networks/deep learning to create detectors. In~\cite{HussainEtAl2021} the authors train a convolutional neural network to detect 91\% of \dDoS{} attacks of a certain kind.

In~\cite{BraunEtAl2020,BraunEtAl2020a}, the authors use an optimization problem in order to identify which part of the system has been attacked.

Another method could be using coalitional control~\cite{ChanfreutEtAl2021} which can potentially cluster healthy agents and detect changes in topologies.

\begin{remark}\label{rem:bounds_and_error}
As stated in~\cite{ArauzEtAl2021} is important to observe that when choosing a bound to detect anomalous behaviors there are false positives associated.
\end{remark}

\subsection{Mitigation/Recovery}
The recovery phase consists in the damage control, changing the system accordingly to reestablish normal behavior.

The main recovery option is immediate substitution of the malfunctioning (or attacked) components by their redundancy (if there is) to prevent further consequences.
Then we fix the component for a later reuse (if possible).

If an immediate substitution is not possible, the control law is changed.
The component is disconnected and we use some robust strategies which can mitigate the effects of the anomalous behavior.
This strategies make the system to behave not exactly as originally intend, but in an satisfactory suboptimal way, until we can restore normalcy.
Examples can be~\cite{MaestreEtAl2018} in which tube-based \mpc{} to robustify against disturbances inside a safety set.
And similarly to~\cite{VelardeEtAl2018}, in~\cite{AnandutaEtAl2018,AnandutaEtAl2019,AnandutaEtAl2020} the authors base their solution on f-local graph robustness, where suspect agents are ignored during the \dmpc{} negotiation.

Another option is to use adaptive strategies which tries to estimate variables or sets and compensate the disturbances caused by the anomalous behaviors~\cite{YangEtAl2022,LuEtAl2020}.

This last strategy is the one used in this work, which will be detailed in the next chapters.
\end{document}

